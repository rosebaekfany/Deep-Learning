{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep Learning Course**\n",
    "\n",
    "## **Loss Functions and Multilayer Perceptrons (MLP)**\n",
    "\n",
    "---\n",
    "\n",
    "### **Student Information:**\n",
    "\n",
    "- **Name:** *Zahra Maleki*\n",
    "- **Student Number:** *400110009*\n",
    "\n",
    "---\n",
    "\n",
    "### **Assignment Overview**\n",
    "\n",
    "In this notebook, we will explore various loss functions used in neural networks, with a specific focus on their role in training **Multilayer Perceptrons (MLPs)**. By the end of this notebook, you will have a deeper understanding of:\n",
    "- Types of loss functions\n",
    "- How loss functions affect the training process\n",
    "- The relationship between loss functions and model optimization in MLPs\n",
    "\n",
    "---\n",
    "\n",
    "### **Table of Contents**\n",
    "\n",
    "1. Introduction to Loss Functions\n",
    "2. Types of Loss Functions\n",
    "3. Multilayer Perceptrons (MLP)\n",
    "4. Implementing Loss Functions in MLP\n",
    "5. Conclusion\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Introduction to Loss Functions \n",
    "\n",
    "In deep learning, **loss functions** play a crucial role in training models by quantifying the difference between the predicted outputs and the actual targets. Selecting the appropriate loss function is essential for the success of your model. In this assay, we will explore various loss functions available in PyTorch, understand their theoretical backgrounds, and provide you with a scaffolded class to experiment with these loss functions.\n",
    "\n",
    "Before begining, let's train a simle MLP model using the **L1Loss** function. We'll return to this model later to experiment with different loss functions. We'll start by importing the necessary libraries and defining the model architecture.\n",
    "\n",
    "First things first, let's talk about **L1Loss**.\n",
    "\n",
    "### 1. L1Loss (`torch.nn.L1Loss`)\n",
    "- **Description:** Also known as Mean Absolute Error (MAE), L1Loss computes the average absolute difference between the predicted values and the target values.\n",
    "- **Use Case:** Suitable for regression tasks where robustness to outliers is desired.\n",
    "\n",
    "Here is the mathematical formulation of L1Loss:\n",
    "\\begin{equation}\n",
    "\\text{L1Loss} = \\frac{1}{n} \\sum_{i=1}^{n} |y_{\\text{pred}_i} - y_{\\text{true}_i}|\n",
    "\\end{equation}\n",
    "\n",
    "Let's implement a simple MLP model using the L1Loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm \n",
    "from torch.nn import NLLLoss, LogSoftmax\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# Don't be courious about Adam, it's just a fancy name for a fancy optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll define a class called `SimpleMLP` that inherits from `nn.Module`. This class can have multiple layers, and we'll use the `nn.Sequential` module to define the layers of the model. The model will have the following architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_hidden_layers=1, last_layer_activation_fn=nn.ReLU):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        # TODO: Define the layers of the MLP\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))  \n",
    "        layers.append(nn.ReLU()) \n",
    "\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim)) \n",
    "            layers.append(nn.ReLU()) \n",
    "    \n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "        if last_layer_activation_fn is not None:\n",
    "            layers.append(last_layer_activation_fn())\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Define the forward pass of the MLP\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define a class called `SimpleMLP_Loss` that has the following architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLPTrainer:\n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train(self, train_loader, num_epochs):\n",
    "        #TODO: Implement the training loop\n",
    "        #Note: You should also print the training loss at each epoch, use tqdm for progress bar\n",
    "        #Note: You should return the training loss at each epoch\n",
    "\n",
    "        training_losses = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            self.model.train() \n",
    "            for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        \n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                \n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item() * inputs.size(0) \n",
    "\n",
    "            epoch_loss /= len(train_loader.dataset)\n",
    "            training_losses.append(epoch_loss)\n",
    "            if (epoch % 10 ==0):\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        return training_losses\n",
    "\n",
    "    def evaluate(self, val_loader):\n",
    "        #TODO: Implement the evaluation loop\n",
    "        #Note: You should return the validation loss and accuracy\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for inputs, targets in val_loader:\n",
    "              \n",
    "                outputs = self.model(inputs)\n",
    "         \n",
    "                loss = self.criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                predictions = (torch.sigmoid(outputs) >= 0.5).float() \n",
    "                correct_predictions += (predictions == targets).sum().item()\n",
    "                total_predictions += targets.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {accuracy * 100:.2f}%\")\n",
    "        \n",
    "        return val_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets test our model using the L1Loss function. You'll use <span style=\"color:red\">*Titanic Dataset*</span> to train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 18/18 [00:00<00:00, 764.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Training Loss: 1.3003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████| 18/18 [00:00<00:00, 675.05it/s]\n",
      "Epoch 3/30: 100%|██████████| 18/18 [00:00<00:00, 1081.94it/s]\n",
      "Epoch 4/30: 100%|██████████| 18/18 [00:00<00:00, 568.80it/s]\n",
      "Epoch 5/30: 100%|██████████| 18/18 [00:00<00:00, 868.50it/s]\n",
      "Epoch 6/30: 100%|██████████| 18/18 [00:00<00:00, 375.86it/s]\n",
      "Epoch 7/30: 100%|██████████| 18/18 [00:00<00:00, 1094.63it/s]\n",
      "Epoch 8/30: 100%|██████████| 18/18 [00:00<00:00, 1124.16it/s]\n",
      "Epoch 9/30: 100%|██████████| 18/18 [00:00<00:00, 728.72it/s]\n",
      "Epoch 10/30: 100%|██████████| 18/18 [00:00<00:00, 570.24it/s]\n",
      "Epoch 11/30: 100%|██████████| 18/18 [00:00<00:00, 445.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/30], Training Loss: 0.4123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|██████████| 18/18 [00:00<00:00, 445.25it/s]\n",
      "Epoch 13/30: 100%|██████████| 18/18 [00:00<00:00, 440.58it/s]\n",
      "Epoch 14/30: 100%|██████████| 18/18 [00:00<00:00, 749.82it/s]\n",
      "Epoch 15/30: 100%|██████████| 18/18 [00:00<00:00, 371.51it/s]\n",
      "Epoch 16/30: 100%|██████████| 18/18 [00:00<00:00, 547.29it/s]\n",
      "Epoch 17/30: 100%|██████████| 18/18 [00:00<00:00, 559.75it/s]\n",
      "Epoch 18/30: 100%|██████████| 18/18 [00:00<00:00, 969.18it/s]\n",
      "Epoch 19/30: 100%|██████████| 18/18 [00:00<00:00, 736.41it/s]\n",
      "Epoch 20/30: 100%|██████████| 18/18 [00:00<00:00, 738.90it/s]\n",
      "Epoch 21/30: 100%|██████████| 18/18 [00:00<00:00, 750.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/30], Training Loss: 0.4102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 100%|██████████| 18/18 [00:00<00:00, 1125.17it/s]\n",
      "Epoch 23/30: 100%|██████████| 18/18 [00:00<00:00, 748.68it/s]\n",
      "Epoch 24/30: 100%|██████████| 18/18 [00:00<00:00, 1122.76it/s]\n",
      "Epoch 25/30: 100%|██████████| 18/18 [00:00<00:00, 1157.14it/s]\n",
      "Epoch 26/30: 100%|██████████| 18/18 [00:00<00:00, 764.97it/s]\n",
      "Epoch 27/30: 100%|██████████| 18/18 [00:00<00:00, 1122.79it/s]\n",
      "Epoch 28/30: 100%|██████████| 18/18 [00:00<00:00, 748.60it/s]\n",
      "Epoch 29/30: 100%|██████████| 18/18 [00:00<00:00, 1104.44it/s]\n",
      "Epoch 30/30: 100%|██████████| 18/18 [00:00<00:00, 1126.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3916, Accuracy: 39.16%\n",
      "Final Validation Loss: 0.3916\n",
      "Final Validation Accuracy: 39.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "train_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "data = pd.read_csv(train_url)\n",
    "\n",
    "# Preprocessing (simple example)\n",
    "data = data[['Pclass', 'Sex', 'Age', 'Fare', 'Survived']].dropna()\n",
    "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "# TODO: Convert the data to PyTorch tensors and create a DataLoader\n",
    "# TODO: Split the data into training and validation sets\n",
    "# TODO: Define the model, criterion, and optimizer\n",
    "\n",
    "X = data[['Pclass', 'Sex', 'Age', 'Fare']].values\n",
    "y = data['Survived'].values\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1) \n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 16\n",
    "output_dim = 1  \n",
    "\n",
    "model = SimpleMLP(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "criterion = nn.L1Loss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "trainer = SimpleMLPTrainer(model, criterion, optimizer)\n",
    "\n",
    "num_epochs = 30\n",
    "training_losses = trainer.train(train_loader, num_epochs=num_epochs)\n",
    "\n",
    "val_loss, val_accuracy = trainer.evaluate(val_loader)\n",
    "\n",
    "print(f\"Final Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <span style=\"color:red; font-size: 26px; font-weight: bold;\">Let's train!</span> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/300: 100%|██████████| 18/18 [00:00<00:00, 662.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Training Loss: 1.2978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/300: 100%|██████████| 18/18 [00:00<00:00, 1087.00it/s]\n",
      "Epoch 3/300: 100%|██████████| 18/18 [00:00<00:00, 748.40it/s]\n",
      "Epoch 4/300: 100%|██████████| 18/18 [00:00<00:00, 720.73it/s]\n",
      "Epoch 5/300: 100%|██████████| 18/18 [00:00<00:00, 1115.36it/s]\n",
      "Epoch 6/300: 100%|██████████| 18/18 [00:00<00:00, 1099.26it/s]\n",
      "Epoch 7/300: 100%|██████████| 18/18 [00:00<00:00, 1122.22it/s]\n",
      "Epoch 8/300: 100%|██████████| 18/18 [00:00<00:00, 813.54it/s]\n",
      "Epoch 9/300: 100%|██████████| 18/18 [00:00<00:00, 1046.25it/s]\n",
      "Epoch 10/300: 100%|██████████| 18/18 [00:00<00:00, 1162.34it/s]\n",
      "Epoch 11/300: 100%|██████████| 18/18 [00:00<00:00, 1089.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/300], Training Loss: 0.5713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/300: 100%|██████████| 18/18 [00:00<00:00, 1051.35it/s]\n",
      "Epoch 13/300: 100%|██████████| 18/18 [00:00<00:00, 1091.43it/s]\n",
      "Epoch 14/300: 100%|██████████| 18/18 [00:00<00:00, 1088.10it/s]\n",
      "Epoch 15/300: 100%|██████████| 18/18 [00:00<00:00, 739.90it/s]\n",
      "Epoch 16/300: 100%|██████████| 18/18 [00:00<00:00, 736.36it/s]\n",
      "Epoch 17/300: 100%|██████████| 18/18 [00:00<00:00, 714.77it/s]\n",
      "Epoch 18/300: 100%|██████████| 18/18 [00:00<00:00, 716.07it/s]\n",
      "Epoch 19/300: 100%|██████████| 18/18 [00:00<00:00, 764.79it/s]\n",
      "Epoch 20/300: 100%|██████████| 18/18 [00:00<00:00, 744.68it/s]\n",
      "Epoch 21/300: 100%|██████████| 18/18 [00:00<00:00, 883.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/300], Training Loss: 0.5414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/300: 100%|██████████| 18/18 [00:00<00:00, 1123.26it/s]\n",
      "Epoch 23/300: 100%|██████████| 18/18 [00:00<00:00, 551.28it/s]\n",
      "Epoch 24/300: 100%|██████████| 18/18 [00:00<00:00, 1121.11it/s]\n",
      "Epoch 25/300: 100%|██████████| 18/18 [00:00<00:00, 1132.03it/s]\n",
      "Epoch 26/300: 100%|██████████| 18/18 [00:00<00:00, 731.52it/s]\n",
      "Epoch 27/300: 100%|██████████| 18/18 [00:00<00:00, 763.05it/s]\n",
      "Epoch 28/300: 100%|██████████| 18/18 [00:00<00:00, 1098.15it/s]\n",
      "Epoch 29/300: 100%|██████████| 18/18 [00:00<00:00, 746.22it/s]\n",
      "Epoch 30/300: 100%|██████████| 18/18 [00:00<00:00, 743.26it/s]\n",
      "Epoch 31/300: 100%|██████████| 18/18 [00:00<00:00, 562.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/300], Training Loss: 0.5293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/300: 100%|██████████| 18/18 [00:00<00:00, 742.11it/s]\n",
      "Epoch 33/300: 100%|██████████| 18/18 [00:00<00:00, 743.99it/s]\n",
      "Epoch 34/300: 100%|██████████| 18/18 [00:00<00:00, 749.49it/s]\n",
      "Epoch 35/300: 100%|██████████| 18/18 [00:00<00:00, 727.04it/s]\n",
      "Epoch 36/300: 100%|██████████| 18/18 [00:00<00:00, 711.88it/s]\n",
      "Epoch 37/300: 100%|██████████| 18/18 [00:00<00:00, 729.61it/s]\n",
      "Epoch 38/300: 100%|██████████| 18/18 [00:00<00:00, 560.49it/s]\n",
      "Epoch 39/300: 100%|██████████| 18/18 [00:00<00:00, 745.93it/s]\n",
      "Epoch 40/300: 100%|██████████| 18/18 [00:00<00:00, 544.75it/s]\n",
      "Epoch 41/300: 100%|██████████| 18/18 [00:00<00:00, 747.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/300], Training Loss: 0.5043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/300: 100%|██████████| 18/18 [00:00<00:00, 339.64it/s]\n",
      "Epoch 43/300: 100%|██████████| 18/18 [00:00<00:00, 735.77it/s]\n",
      "Epoch 44/300: 100%|██████████| 18/18 [00:00<00:00, 734.42it/s]\n",
      "Epoch 45/300: 100%|██████████| 18/18 [00:00<00:00, 747.63it/s]\n",
      "Epoch 46/300: 100%|██████████| 18/18 [00:00<00:00, 738.13it/s]\n",
      "Epoch 47/300: 100%|██████████| 18/18 [00:00<00:00, 749.35it/s]\n",
      "Epoch 48/300: 100%|██████████| 18/18 [00:00<00:00, 1092.09it/s]\n",
      "Epoch 49/300: 100%|██████████| 18/18 [00:00<00:00, 739.48it/s]\n",
      "Epoch 50/300: 100%|██████████| 18/18 [00:00<00:00, 734.87it/s]\n",
      "Epoch 51/300: 100%|██████████| 18/18 [00:00<00:00, 742.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/300], Training Loss: 0.4811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/300: 100%|██████████| 18/18 [00:00<00:00, 775.61it/s]\n",
      "Epoch 53/300: 100%|██████████| 18/18 [00:00<00:00, 1091.24it/s]\n",
      "Epoch 54/300: 100%|██████████| 18/18 [00:00<00:00, 782.99it/s]\n",
      "Epoch 55/300: 100%|██████████| 18/18 [00:00<00:00, 715.15it/s]\n",
      "Epoch 56/300: 100%|██████████| 18/18 [00:00<00:00, 742.68it/s]\n",
      "Epoch 57/300: 100%|██████████| 18/18 [00:00<00:00, 680.48it/s]\n",
      "Epoch 58/300: 100%|██████████| 18/18 [00:00<00:00, 1115.62it/s]\n",
      "Epoch 59/300: 100%|██████████| 18/18 [00:00<00:00, 1084.25it/s]\n",
      "Epoch 60/300: 100%|██████████| 18/18 [00:00<00:00, 1121.09it/s]\n",
      "Epoch 61/300: 100%|██████████| 18/18 [00:00<00:00, 627.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/300], Training Loss: 0.4692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/300: 100%|██████████| 18/18 [00:00<00:00, 1124.11it/s]\n",
      "Epoch 63/300: 100%|██████████| 18/18 [00:00<00:00, 1103.89it/s]\n",
      "Epoch 64/300: 100%|██████████| 18/18 [00:00<00:00, 1102.97it/s]\n",
      "Epoch 65/300: 100%|██████████| 18/18 [00:00<00:00, 734.10it/s]\n",
      "Epoch 66/300: 100%|██████████| 18/18 [00:00<00:00, 1096.87it/s]\n",
      "Epoch 67/300: 100%|██████████| 18/18 [00:00<00:00, 1149.02it/s]\n",
      "Epoch 68/300: 100%|██████████| 18/18 [00:00<00:00, 1120.92it/s]\n",
      "Epoch 69/300: 100%|██████████| 18/18 [00:00<00:00, 561.69it/s]\n",
      "Epoch 70/300: 100%|██████████| 18/18 [00:00<00:00, 743.80it/s]\n",
      "Epoch 71/300: 100%|██████████| 18/18 [00:00<00:00, 744.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/300], Training Loss: 0.4643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/300: 100%|██████████| 18/18 [00:00<00:00, 441.18it/s]\n",
      "Epoch 73/300: 100%|██████████| 18/18 [00:00<00:00, 740.76it/s]\n",
      "Epoch 74/300: 100%|██████████| 18/18 [00:00<00:00, 1078.38it/s]\n",
      "Epoch 75/300: 100%|██████████| 18/18 [00:00<00:00, 1128.97it/s]\n",
      "Epoch 76/300: 100%|██████████| 18/18 [00:00<00:00, 1112.24it/s]\n",
      "Epoch 77/300: 100%|██████████| 18/18 [00:00<00:00, 1065.79it/s]\n",
      "Epoch 78/300: 100%|██████████| 18/18 [00:00<00:00, 739.50it/s]\n",
      "Epoch 79/300: 100%|██████████| 18/18 [00:00<00:00, 1072.45it/s]\n",
      "Epoch 80/300: 100%|██████████| 18/18 [00:00<00:00, 1092.73it/s]\n",
      "Epoch 81/300: 100%|██████████| 18/18 [00:00<00:00, 1061.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [81/300], Training Loss: 0.4588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/300: 100%|██████████| 18/18 [00:00<00:00, 976.69it/s]\n",
      "Epoch 83/300: 100%|██████████| 18/18 [00:00<00:00, 830.80it/s]\n",
      "Epoch 84/300: 100%|██████████| 18/18 [00:00<00:00, 704.38it/s]\n",
      "Epoch 85/300: 100%|██████████| 18/18 [00:00<00:00, 1114.22it/s]\n",
      "Epoch 86/300: 100%|██████████| 18/18 [00:00<00:00, 1098.03it/s]\n",
      "Epoch 87/300: 100%|██████████| 18/18 [00:00<00:00, 885.28it/s]\n",
      "Epoch 88/300: 100%|██████████| 18/18 [00:00<00:00, 737.12it/s]\n",
      "Epoch 89/300: 100%|██████████| 18/18 [00:00<00:00, 1093.47it/s]\n",
      "Epoch 90/300: 100%|██████████| 18/18 [00:00<00:00, 743.15it/s]\n",
      "Epoch 91/300: 100%|██████████| 18/18 [00:00<00:00, 1088.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [91/300], Training Loss: 0.4550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/300: 100%|██████████| 18/18 [00:00<00:00, 1057.65it/s]\n",
      "Epoch 93/300: 100%|██████████| 18/18 [00:00<00:00, 1101.25it/s]\n",
      "Epoch 94/300: 100%|██████████| 18/18 [00:00<00:00, 1080.98it/s]\n",
      "Epoch 95/300: 100%|██████████| 18/18 [00:00<00:00, 1086.73it/s]\n",
      "Epoch 96/300: 100%|██████████| 18/18 [00:00<00:00, 1087.59it/s]\n",
      "Epoch 97/300: 100%|██████████| 18/18 [00:00<00:00, 1069.76it/s]\n",
      "Epoch 98/300: 100%|██████████| 18/18 [00:00<00:00, 1103.05it/s]\n",
      "Epoch 99/300: 100%|██████████| 18/18 [00:00<00:00, 1094.06it/s]\n",
      "Epoch 100/300: 100%|██████████| 18/18 [00:00<00:00, 944.53it/s]\n",
      "Epoch 101/300: 100%|██████████| 18/18 [00:00<00:00, 1233.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [101/300], Training Loss: 0.4508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 102/300: 100%|██████████| 18/18 [00:00<00:00, 1095.06it/s]\n",
      "Epoch 103/300: 100%|██████████| 18/18 [00:00<00:00, 738.35it/s]\n",
      "Epoch 104/300: 100%|██████████| 18/18 [00:00<00:00, 1104.72it/s]\n",
      "Epoch 105/300: 100%|██████████| 18/18 [00:00<00:00, 445.24it/s]\n",
      "Epoch 106/300: 100%|██████████| 18/18 [00:00<00:00, 743.20it/s]\n",
      "Epoch 107/300: 100%|██████████| 18/18 [00:00<00:00, 709.53it/s]\n",
      "Epoch 108/300: 100%|██████████| 18/18 [00:00<00:00, 1471.26it/s]\n",
      "Epoch 109/300: 100%|██████████| 18/18 [00:00<00:00, 1901.22it/s]\n",
      "Epoch 110/300: 100%|██████████| 18/18 [00:00<00:00, 1095.33it/s]\n",
      "Epoch 111/300: 100%|██████████| 18/18 [00:00<00:00, 1085.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [111/300], Training Loss: 0.4528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 112/300: 100%|██████████| 18/18 [00:00<00:00, 2108.40it/s]\n",
      "Epoch 113/300: 100%|██████████| 18/18 [00:00<00:00, 1110.73it/s]\n",
      "Epoch 114/300: 100%|██████████| 18/18 [00:00<00:00, 1145.29it/s]\n",
      "Epoch 115/300: 100%|██████████| 18/18 [00:00<00:00, 1081.56it/s]\n",
      "Epoch 116/300: 100%|██████████| 18/18 [00:00<00:00, 1096.88it/s]\n",
      "Epoch 117/300: 100%|██████████| 18/18 [00:00<00:00, 1067.03it/s]\n",
      "Epoch 118/300: 100%|██████████| 18/18 [00:00<00:00, 1104.75it/s]\n",
      "Epoch 119/300: 100%|██████████| 18/18 [00:00<00:00, 2241.35it/s]\n",
      "Epoch 120/300: 100%|██████████| 18/18 [00:00<00:00, 1075.49it/s]\n",
      "Epoch 121/300: 100%|██████████| 18/18 [00:00<00:00, 1114.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [121/300], Training Loss: 0.4475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 122/300: 100%|██████████| 18/18 [00:00<00:00, 1074.07it/s]\n",
      "Epoch 123/300: 100%|██████████| 18/18 [00:00<00:00, 757.65it/s]\n",
      "Epoch 124/300: 100%|██████████| 18/18 [00:00<00:00, 1108.56it/s]\n",
      "Epoch 125/300: 100%|██████████| 18/18 [00:00<00:00, 1148.72it/s]\n",
      "Epoch 126/300: 100%|██████████| 18/18 [00:00<00:00, 718.95it/s]\n",
      "Epoch 127/300: 100%|██████████| 18/18 [00:00<00:00, 564.61it/s]\n",
      "Epoch 128/300: 100%|██████████| 18/18 [00:00<00:00, 736.59it/s]\n",
      "Epoch 129/300: 100%|██████████| 18/18 [00:00<00:00, 728.11it/s]\n",
      "Epoch 130/300: 100%|██████████| 18/18 [00:00<00:00, 746.33it/s]\n",
      "Epoch 131/300: 100%|██████████| 18/18 [00:00<00:00, 1094.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [131/300], Training Loss: 0.4462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 132/300: 100%|██████████| 18/18 [00:00<00:00, 735.20it/s]\n",
      "Epoch 133/300: 100%|██████████| 18/18 [00:00<00:00, 1101.28it/s]\n",
      "Epoch 134/300: 100%|██████████| 18/18 [00:00<00:00, 1168.76it/s]\n",
      "Epoch 135/300: 100%|██████████| 18/18 [00:00<00:00, 726.89it/s]\n",
      "Epoch 136/300: 100%|██████████| 18/18 [00:00<00:00, 1115.21it/s]\n",
      "Epoch 137/300: 100%|██████████| 18/18 [00:00<00:00, 2184.85it/s]\n",
      "Epoch 138/300: 100%|██████████| 18/18 [00:00<00:00, 447.70it/s]\n",
      "Epoch 139/300: 100%|██████████| 18/18 [00:00<00:00, 739.36it/s]\n",
      "Epoch 140/300: 100%|██████████| 18/18 [00:00<00:00, 1071.54it/s]\n",
      "Epoch 141/300: 100%|██████████| 18/18 [00:00<00:00, 1112.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [141/300], Training Loss: 0.4448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 142/300: 100%|██████████| 18/18 [00:00<00:00, 1117.21it/s]\n",
      "Epoch 143/300: 100%|██████████| 18/18 [00:00<00:00, 1104.09it/s]\n",
      "Epoch 144/300: 100%|██████████| 18/18 [00:00<00:00, 1106.79it/s]\n",
      "Epoch 145/300: 100%|██████████| 18/18 [00:00<00:00, 1975.91it/s]\n",
      "Epoch 146/300: 100%|██████████| 18/18 [00:00<00:00, 1118.48it/s]\n",
      "Epoch 147/300: 100%|██████████| 18/18 [00:00<00:00, 1078.77it/s]\n",
      "Epoch 148/300: 100%|██████████| 18/18 [00:00<00:00, 2064.46it/s]\n",
      "Epoch 149/300: 100%|██████████| 18/18 [00:00<00:00, 1091.93it/s]\n",
      "Epoch 150/300: 100%|██████████| 18/18 [00:00<00:00, 1086.47it/s]\n",
      "Epoch 151/300: 100%|██████████| 18/18 [00:00<00:00, 1073.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [151/300], Training Loss: 0.4432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 152/300: 100%|██████████| 18/18 [00:00<00:00, 1124.75it/s]\n",
      "Epoch 153/300: 100%|██████████| 18/18 [00:00<00:00, 1086.78it/s]\n",
      "Epoch 154/300: 100%|██████████| 18/18 [00:00<00:00, 1160.07it/s]\n",
      "Epoch 155/300: 100%|██████████| 18/18 [00:00<00:00, 1120.42it/s]\n",
      "Epoch 156/300: 100%|██████████| 18/18 [00:00<00:00, 685.41it/s]\n",
      "Epoch 157/300: 100%|██████████| 18/18 [00:00<00:00, 1095.04it/s]\n",
      "Epoch 158/300: 100%|██████████| 18/18 [00:00<00:00, 688.66it/s]\n",
      "Epoch 159/300: 100%|██████████| 18/18 [00:00<00:00, 759.88it/s]\n",
      "Epoch 160/300: 100%|██████████| 18/18 [00:00<00:00, 664.64it/s]\n",
      "Epoch 161/300: 100%|██████████| 18/18 [00:00<00:00, 729.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [161/300], Training Loss: 0.4405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 162/300: 100%|██████████| 18/18 [00:00<00:00, 749.03it/s]\n",
      "Epoch 163/300: 100%|██████████| 18/18 [00:00<00:00, 735.87it/s]\n",
      "Epoch 164/300: 100%|██████████| 18/18 [00:00<00:00, 749.91it/s]\n",
      "Epoch 165/300: 100%|██████████| 18/18 [00:00<00:00, 759.65it/s]\n",
      "Epoch 166/300: 100%|██████████| 18/18 [00:00<00:00, 806.19it/s]\n",
      "Epoch 167/300: 100%|██████████| 18/18 [00:00<00:00, 1096.39it/s]\n",
      "Epoch 168/300: 100%|██████████| 18/18 [00:00<00:00, 2366.77it/s]\n",
      "Epoch 169/300: 100%|██████████| 18/18 [00:00<00:00, 562.92it/s]\n",
      "Epoch 170/300: 100%|██████████| 18/18 [00:00<00:00, 1076.09it/s]\n",
      "Epoch 171/300: 100%|██████████| 18/18 [00:00<00:00, 758.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [171/300], Training Loss: 0.4420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 172/300: 100%|██████████| 18/18 [00:00<00:00, 546.91it/s]\n",
      "Epoch 173/300: 100%|██████████| 18/18 [00:00<00:00, 1118.80it/s]\n",
      "Epoch 174/300: 100%|██████████| 18/18 [00:00<00:00, 1092.88it/s]\n",
      "Epoch 175/300: 100%|██████████| 18/18 [00:00<00:00, 1058.26it/s]\n",
      "Epoch 176/300: 100%|██████████| 18/18 [00:00<00:00, 746.33it/s]\n",
      "Epoch 177/300: 100%|██████████| 18/18 [00:00<00:00, 746.58it/s]\n",
      "Epoch 178/300: 100%|██████████| 18/18 [00:00<00:00, 747.51it/s]\n",
      "Epoch 179/300: 100%|██████████| 18/18 [00:00<00:00, 454.49it/s]\n",
      "Epoch 180/300: 100%|██████████| 18/18 [00:00<00:00, 460.26it/s]\n",
      "Epoch 181/300: 100%|██████████| 18/18 [00:00<00:00, 553.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [181/300], Training Loss: 0.4414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 182/300: 100%|██████████| 18/18 [00:00<00:00, 538.30it/s]\n",
      "Epoch 183/300: 100%|██████████| 18/18 [00:00<00:00, 749.14it/s]\n",
      "Epoch 184/300: 100%|██████████| 18/18 [00:00<00:00, 1099.58it/s]\n",
      "Epoch 185/300: 100%|██████████| 18/18 [00:00<00:00, 1080.43it/s]\n",
      "Epoch 186/300: 100%|██████████| 18/18 [00:00<00:00, 1117.42it/s]\n",
      "Epoch 187/300: 100%|██████████| 18/18 [00:00<00:00, 1076.18it/s]\n",
      "Epoch 188/300: 100%|██████████| 18/18 [00:00<00:00, 1173.94it/s]\n",
      "Epoch 189/300: 100%|██████████| 18/18 [00:00<00:00, 1106.25it/s]\n",
      "Epoch 190/300: 100%|██████████| 18/18 [00:00<00:00, 1136.87it/s]\n",
      "Epoch 191/300: 100%|██████████| 18/18 [00:00<00:00, 1115.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [191/300], Training Loss: 0.4426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 192/300: 100%|██████████| 18/18 [00:00<00:00, 1148.83it/s]\n",
      "Epoch 193/300: 100%|██████████| 18/18 [00:00<00:00, 1097.19it/s]\n",
      "Epoch 194/300: 100%|██████████| 18/18 [00:00<00:00, 1103.01it/s]\n",
      "Epoch 195/300: 100%|██████████| 18/18 [00:00<00:00, 1082.63it/s]\n",
      "Epoch 196/300: 100%|██████████| 18/18 [00:00<00:00, 1095.96it/s]\n",
      "Epoch 197/300: 100%|██████████| 18/18 [00:00<00:00, 1097.46it/s]\n",
      "Epoch 198/300: 100%|██████████| 18/18 [00:00<00:00, 2060.58it/s]\n",
      "Epoch 199/300: 100%|██████████| 18/18 [00:00<00:00, 1093.50it/s]\n",
      "Epoch 200/300: 100%|██████████| 18/18 [00:00<00:00, 1094.91it/s]\n",
      "Epoch 201/300: 100%|██████████| 18/18 [00:00<00:00, 1078.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [201/300], Training Loss: 0.4435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 202/300: 100%|██████████| 18/18 [00:00<00:00, 1066.59it/s]\n",
      "Epoch 203/300: 100%|██████████| 18/18 [00:00<00:00, 1086.83it/s]\n",
      "Epoch 204/300: 100%|██████████| 18/18 [00:00<00:00, 1100.03it/s]\n",
      "Epoch 205/300: 100%|██████████| 18/18 [00:00<00:00, 1091.63it/s]\n",
      "Epoch 206/300: 100%|██████████| 18/18 [00:00<00:00, 1463.90it/s]\n",
      "Epoch 207/300: 100%|██████████| 18/18 [00:00<00:00, 551.10it/s]\n",
      "Epoch 208/300: 100%|██████████| 18/18 [00:00<00:00, 692.63it/s]\n",
      "Epoch 209/300: 100%|██████████| 18/18 [00:00<00:00, 931.06it/s]\n",
      "Epoch 210/300: 100%|██████████| 18/18 [00:00<00:00, 2157.32it/s]\n",
      "Epoch 211/300: 100%|██████████| 18/18 [00:00<00:00, 2087.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [211/300], Training Loss: 0.4373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 212/300: 100%|██████████| 18/18 [00:00<00:00, 1089.07it/s]\n",
      "Epoch 213/300: 100%|██████████| 18/18 [00:00<00:00, 1079.63it/s]\n",
      "Epoch 214/300: 100%|██████████| 18/18 [00:00<00:00, 1087.34it/s]\n",
      "Epoch 215/300: 100%|██████████| 18/18 [00:00<00:00, 1116.02it/s]\n",
      "Epoch 216/300: 100%|██████████| 18/18 [00:00<00:00, 1079.18it/s]\n",
      "Epoch 217/300: 100%|██████████| 18/18 [00:00<00:00, 1193.30it/s]\n",
      "Epoch 218/300: 100%|██████████| 18/18 [00:00<00:00, 718.74it/s]\n",
      "Epoch 219/300: 100%|██████████| 18/18 [00:00<00:00, 1090.41it/s]\n",
      "Epoch 220/300: 100%|██████████| 18/18 [00:00<00:00, 985.11it/s]\n",
      "Epoch 221/300: 100%|██████████| 18/18 [00:00<00:00, 2365.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [221/300], Training Loss: 0.4358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 222/300: 100%|██████████| 18/18 [00:00<00:00, 1149.88it/s]\n",
      "Epoch 223/300: 100%|██████████| 18/18 [00:00<00:00, 1097.95it/s]\n",
      "Epoch 224/300: 100%|██████████| 18/18 [00:00<00:00, 1108.40it/s]\n",
      "Epoch 225/300: 100%|██████████| 18/18 [00:00<00:00, 860.36it/s]\n",
      "Epoch 226/300: 100%|██████████| 18/18 [00:00<00:00, 1147.80it/s]\n",
      "Epoch 227/300: 100%|██████████| 18/18 [00:00<00:00, 734.35it/s]\n",
      "Epoch 228/300: 100%|██████████| 18/18 [00:00<00:00, 1150.24it/s]\n",
      "Epoch 229/300: 100%|██████████| 18/18 [00:00<00:00, 1120.47it/s]\n",
      "Epoch 230/300: 100%|██████████| 18/18 [00:00<00:00, 1005.33it/s]\n",
      "Epoch 231/300: 100%|██████████| 18/18 [00:00<00:00, 777.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [231/300], Training Loss: 0.4366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 232/300: 100%|██████████| 18/18 [00:00<00:00, 1100.51it/s]\n",
      "Epoch 233/300: 100%|██████████| 18/18 [00:00<00:00, 732.56it/s]\n",
      "Epoch 234/300: 100%|██████████| 18/18 [00:00<00:00, 1018.83it/s]\n",
      "Epoch 235/300: 100%|██████████| 18/18 [00:00<00:00, 562.31it/s]\n",
      "Epoch 236/300: 100%|██████████| 18/18 [00:00<00:00, 292.61it/s]\n",
      "Epoch 237/300: 100%|██████████| 18/18 [00:00<00:00, 562.24it/s]\n",
      "Epoch 238/300: 100%|██████████| 18/18 [00:00<00:00, 1125.03it/s]\n",
      "Epoch 239/300: 100%|██████████| 18/18 [00:00<00:00, 746.82it/s]\n",
      "Epoch 240/300: 100%|██████████| 18/18 [00:00<00:00, 570.83it/s]\n",
      "Epoch 241/300: 100%|██████████| 18/18 [00:00<00:00, 562.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [241/300], Training Loss: 0.4366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 242/300: 100%|██████████| 18/18 [00:00<00:00, 745.36it/s]\n",
      "Epoch 243/300: 100%|██████████| 18/18 [00:00<00:00, 560.85it/s]\n",
      "Epoch 244/300: 100%|██████████| 18/18 [00:00<00:00, 718.44it/s]\n",
      "Epoch 245/300: 100%|██████████| 18/18 [00:00<00:00, 734.67it/s]\n",
      "Epoch 246/300: 100%|██████████| 18/18 [00:00<00:00, 561.87it/s]\n",
      "Epoch 247/300: 100%|██████████| 18/18 [00:00<00:00, 742.81it/s]\n",
      "Epoch 248/300: 100%|██████████| 18/18 [00:00<00:00, 562.34it/s]\n",
      "Epoch 249/300: 100%|██████████| 18/18 [00:00<00:00, 557.90it/s]\n",
      "Epoch 250/300: 100%|██████████| 18/18 [00:00<00:00, 750.35it/s]\n",
      "Epoch 251/300: 100%|██████████| 18/18 [00:00<00:00, 750.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [251/300], Training Loss: 0.4356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 252/300: 100%|██████████| 18/18 [00:00<00:00, 559.65it/s]\n",
      "Epoch 253/300: 100%|██████████| 18/18 [00:00<00:00, 699.54it/s]\n",
      "Epoch 254/300: 100%|██████████| 18/18 [00:00<00:00, 709.48it/s]\n",
      "Epoch 255/300: 100%|██████████| 18/18 [00:00<00:00, 699.43it/s]\n",
      "Epoch 256/300: 100%|██████████| 18/18 [00:00<00:00, 600.46it/s]\n",
      "Epoch 257/300: 100%|██████████| 18/18 [00:00<00:00, 764.42it/s]\n",
      "Epoch 258/300: 100%|██████████| 18/18 [00:00<00:00, 1125.13it/s]\n",
      "Epoch 259/300: 100%|██████████| 18/18 [00:00<00:00, 561.72it/s]\n",
      "Epoch 260/300: 100%|██████████| 18/18 [00:00<00:00, 562.78it/s]\n",
      "Epoch 261/300: 100%|██████████| 18/18 [00:00<00:00, 562.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [261/300], Training Loss: 0.4358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 262/300: 100%|██████████| 18/18 [00:00<00:00, 781.42it/s]\n",
      "Epoch 263/300: 100%|██████████| 18/18 [00:00<00:00, 447.56it/s]\n",
      "Epoch 264/300: 100%|██████████| 18/18 [00:00<00:00, 551.50it/s]\n",
      "Epoch 265/300: 100%|██████████| 18/18 [00:00<00:00, 449.62it/s]\n",
      "Epoch 266/300: 100%|██████████| 18/18 [00:00<00:00, 562.77it/s]\n",
      "Epoch 267/300: 100%|██████████| 18/18 [00:00<00:00, 733.34it/s]\n",
      "Epoch 268/300: 100%|██████████| 18/18 [00:00<00:00, 750.52it/s]\n",
      "Epoch 269/300: 100%|██████████| 18/18 [00:00<00:00, 750.17it/s]\n",
      "Epoch 270/300: 100%|██████████| 18/18 [00:00<00:00, 628.53it/s]\n",
      "Epoch 271/300: 100%|██████████| 18/18 [00:00<00:00, 1120.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [271/300], Training Loss: 0.4372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 272/300: 100%|██████████| 18/18 [00:00<00:00, 749.18it/s]\n",
      "Epoch 273/300: 100%|██████████| 18/18 [00:00<00:00, 717.49it/s]\n",
      "Epoch 274/300: 100%|██████████| 18/18 [00:00<00:00, 549.75it/s]\n",
      "Epoch 275/300: 100%|██████████| 18/18 [00:00<00:00, 544.39it/s]\n",
      "Epoch 276/300: 100%|██████████| 18/18 [00:00<00:00, 748.62it/s]\n",
      "Epoch 277/300: 100%|██████████| 18/18 [00:00<00:00, 1121.41it/s]\n",
      "Epoch 278/300: 100%|██████████| 18/18 [00:00<00:00, 1195.73it/s]\n",
      "Epoch 279/300: 100%|██████████| 18/18 [00:00<00:00, 1199.57it/s]\n",
      "Epoch 280/300: 100%|██████████| 18/18 [00:00<00:00, 561.86it/s]\n",
      "Epoch 281/300: 100%|██████████| 18/18 [00:00<00:00, 635.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [281/300], Training Loss: 0.4351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 282/300: 100%|██████████| 18/18 [00:00<00:00, 747.78it/s]\n",
      "Epoch 283/300: 100%|██████████| 18/18 [00:00<00:00, 718.62it/s]\n",
      "Epoch 284/300: 100%|██████████| 18/18 [00:00<00:00, 737.75it/s]\n",
      "Epoch 285/300: 100%|██████████| 18/18 [00:00<00:00, 561.95it/s]\n",
      "Epoch 286/300: 100%|██████████| 18/18 [00:00<00:00, 1085.37it/s]\n",
      "Epoch 287/300: 100%|██████████| 18/18 [00:00<00:00, 748.73it/s]\n",
      "Epoch 288/300: 100%|██████████| 18/18 [00:00<00:00, 661.48it/s]\n",
      "Epoch 289/300: 100%|██████████| 18/18 [00:00<00:00, 737.52it/s]\n",
      "Epoch 290/300: 100%|██████████| 18/18 [00:00<00:00, 562.06it/s]\n",
      "Epoch 291/300: 100%|██████████| 18/18 [00:00<00:00, 561.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [291/300], Training Loss: 0.4376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 292/300: 100%|██████████| 18/18 [00:00<00:00, 1122.67it/s]\n",
      "Epoch 293/300: 100%|██████████| 18/18 [00:00<00:00, 764.94it/s]\n",
      "Epoch 294/300: 100%|██████████| 18/18 [00:00<00:00, 748.27it/s]\n",
      "Epoch 295/300: 100%|██████████| 18/18 [00:00<00:00, 751.19it/s]\n",
      "Epoch 296/300: 100%|██████████| 18/18 [00:00<00:00, 854.99it/s]\n",
      "Epoch 297/300: 100%|██████████| 18/18 [00:00<00:00, 1066.42it/s]\n",
      "Epoch 298/300: 100%|██████████| 18/18 [00:00<00:00, 1154.43it/s]\n",
      "Epoch 299/300: 100%|██████████| 18/18 [00:00<00:00, 1125.69it/s]\n",
      "Epoch 300/300: 100%|██████████| 18/18 [00:00<00:00, 1116.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5122, Accuracy: 73.43%\n",
      "Final Validation Loss: 0.5122\n",
      "Final Validation Accuracy: 73.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import L1Loss\n",
    "\n",
    "# TODO: Train the model\n",
    "\n",
    "# TODO: Evaluate the model\n",
    "\n",
    "num_epochs = 300\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 16\n",
    "output_dim = 1  \n",
    "\n",
    "model = SimpleMLP(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, last_layer_activation_fn=None)\n",
    "criterion = nn.BCEWithLogitsLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "trainer = SimpleMLPTrainer(model, criterion, optimizer)\n",
    "\n",
    "training_losses = trainer.train(train_loader, num_epochs=num_epochs)\n",
    "\n",
    "val_loss, val_accuracy = trainer.evaluate(val_loader)\n",
    "\n",
    "print(f\"Final Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Types of Loss Functions\n",
    "\n",
    "PyTorch offers a variety of built-in loss functions tailored for different types of problems, such as regression, classification, and more. Below, we discuss several commonly used loss functions, their theoretical foundations, and typical use cases.\n",
    "\n",
    "### 2. MSELoss (`torch.nn.MSELoss`)\n",
    "- **Description:** Mean Squared Error (MSE) calculates the average of the squares of the differences between predicted and target values.\n",
    "- **Use Case:** Commonly used in regression problems where larger errors are significantly penalized.\n",
    "\n",
    "Here is boring math stuff for MSE:\n",
    "\\begin{equation}\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^{2}\n",
    "\\end{equation}\n",
    "\n",
    "<span style=\"color:red; font-size: 18px; font-weight: bold;\">Warning:</span> Don't forget to reinitialize the model before experimenting with different loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/700: 100%|██████████| 18/18 [00:00<00:00, 704.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/700], Training Loss: 3.7033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/700: 100%|██████████| 18/18 [00:00<00:00, 749.66it/s]\n",
      "Epoch 3/700: 100%|██████████| 18/18 [00:00<00:00, 1116.23it/s]\n",
      "Epoch 4/700: 100%|██████████| 18/18 [00:00<00:00, 1125.01it/s]\n",
      "Epoch 5/700: 100%|██████████| 18/18 [00:00<00:00, 748.40it/s]\n",
      "Epoch 6/700: 100%|██████████| 18/18 [00:00<00:00, 1061.83it/s]\n",
      "Epoch 7/700: 100%|██████████| 18/18 [00:00<00:00, 2241.54it/s]\n",
      "Epoch 8/700: 100%|██████████| 18/18 [00:00<00:00, 1112.50it/s]\n",
      "Epoch 9/700: 100%|██████████| 18/18 [00:00<00:00, 704.35it/s]\n",
      "Epoch 10/700: 100%|██████████| 18/18 [00:00<00:00, 798.90it/s]\n",
      "Epoch 11/700: 100%|██████████| 18/18 [00:00<00:00, 1124.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/700], Training Loss: 0.2424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/700: 100%|██████████| 18/18 [00:00<00:00, 1124.95it/s]\n",
      "Epoch 13/700: 100%|██████████| 18/18 [00:00<00:00, 1116.78it/s]\n",
      "Epoch 14/700: 100%|██████████| 18/18 [00:00<00:00, 1196.85it/s]\n",
      "Epoch 15/700: 100%|██████████| 18/18 [00:00<00:00, 637.51it/s]\n",
      "Epoch 16/700: 100%|██████████| 18/18 [00:00<00:00, 1052.89it/s]\n",
      "Epoch 17/700: 100%|██████████| 18/18 [00:00<00:00, 1125.06it/s]\n",
      "Epoch 18/700: 100%|██████████| 18/18 [00:00<00:00, 747.93it/s]\n",
      "Epoch 19/700: 100%|██████████| 18/18 [00:00<00:00, 1314.53it/s]\n",
      "Epoch 20/700: 100%|██████████| 18/18 [00:00<00:00, 764.17it/s]\n",
      "Epoch 21/700: 100%|██████████| 18/18 [00:00<00:00, 750.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/700], Training Loss: 0.2151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/700: 100%|██████████| 18/18 [00:00<00:00, 553.90it/s]\n",
      "Epoch 23/700: 100%|██████████| 18/18 [00:00<00:00, 733.15it/s]\n",
      "Epoch 24/700: 100%|██████████| 18/18 [00:00<00:00, 764.85it/s]\n",
      "Epoch 25/700: 100%|██████████| 18/18 [00:00<00:00, 691.84it/s]\n",
      "Epoch 26/700: 100%|██████████| 18/18 [00:00<00:00, 805.04it/s]\n",
      "Epoch 27/700: 100%|██████████| 18/18 [00:00<00:00, 690.95it/s]\n",
      "Epoch 28/700: 100%|██████████| 18/18 [00:00<00:00, 1075.89it/s]\n",
      "Epoch 29/700: 100%|██████████| 18/18 [00:00<00:00, 699.54it/s]\n",
      "Epoch 30/700: 100%|██████████| 18/18 [00:00<00:00, 787.32it/s]\n",
      "Epoch 31/700: 100%|██████████| 18/18 [00:00<00:00, 1170.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/700], Training Loss: 0.2010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/700: 100%|██████████| 18/18 [00:00<00:00, 723.22it/s]\n",
      "Epoch 33/700: 100%|██████████| 18/18 [00:00<00:00, 454.67it/s]\n",
      "Epoch 34/700: 100%|██████████| 18/18 [00:00<00:00, 557.36it/s]\n",
      "Epoch 35/700: 100%|██████████| 18/18 [00:00<00:00, 746.64it/s]\n",
      "Epoch 36/700: 100%|██████████| 18/18 [00:00<00:00, 1119.33it/s]\n",
      "Epoch 37/700: 100%|██████████| 18/18 [00:00<00:00, 714.55it/s]\n",
      "Epoch 38/700: 100%|██████████| 18/18 [00:00<00:00, 1159.36it/s]\n",
      "Epoch 39/700: 100%|██████████| 18/18 [00:00<00:00, 749.79it/s]\n",
      "Epoch 40/700: 100%|██████████| 18/18 [00:00<00:00, 739.41it/s]\n",
      "Epoch 41/700: 100%|██████████| 18/18 [00:00<00:00, 702.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/700], Training Loss: 0.1808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/700: 100%|██████████| 18/18 [00:00<00:00, 781.85it/s]\n",
      "Epoch 43/700: 100%|██████████| 18/18 [00:00<00:00, 1123.07it/s]\n",
      "Epoch 44/700: 100%|██████████| 18/18 [00:00<00:00, 750.77it/s]\n",
      "Epoch 45/700: 100%|██████████| 18/18 [00:00<00:00, 747.48it/s]\n",
      "Epoch 46/700: 100%|██████████| 18/18 [00:00<00:00, 1125.01it/s]\n",
      "Epoch 47/700: 100%|██████████| 18/18 [00:00<00:00, 749.96it/s]\n",
      "Epoch 48/700: 100%|██████████| 18/18 [00:00<00:00, 749.09it/s]\n",
      "Epoch 49/700: 100%|██████████| 18/18 [00:00<00:00, 748.63it/s]\n",
      "Epoch 50/700: 100%|██████████| 18/18 [00:00<00:00, 1156.43it/s]\n",
      "Epoch 51/700: 100%|██████████| 18/18 [00:00<00:00, 1126.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/700], Training Loss: 0.1680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/700: 100%|██████████| 18/18 [00:00<00:00, 1112.06it/s]\n",
      "Epoch 53/700: 100%|██████████| 18/18 [00:00<00:00, 748.36it/s]\n",
      "Epoch 54/700: 100%|██████████| 18/18 [00:00<00:00, 1120.06it/s]\n",
      "Epoch 55/700: 100%|██████████| 18/18 [00:00<00:00, 1095.18it/s]\n",
      "Epoch 56/700: 100%|██████████| 18/18 [00:00<00:00, 548.79it/s]\n",
      "Epoch 57/700: 100%|██████████| 18/18 [00:00<00:00, 682.10it/s]\n",
      "Epoch 58/700: 100%|██████████| 18/18 [00:00<00:00, 557.12it/s]\n",
      "Epoch 59/700: 100%|██████████| 18/18 [00:00<00:00, 1122.44it/s]\n",
      "Epoch 60/700: 100%|██████████| 18/18 [00:00<00:00, 1156.94it/s]\n",
      "Epoch 61/700: 100%|██████████| 18/18 [00:00<00:00, 561.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/700], Training Loss: 0.1619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/700: 100%|██████████| 18/18 [00:00<00:00, 446.78it/s]\n",
      "Epoch 63/700: 100%|██████████| 18/18 [00:00<00:00, 570.24it/s]\n",
      "Epoch 64/700: 100%|██████████| 18/18 [00:00<00:00, 1122.62it/s]\n",
      "Epoch 65/700: 100%|██████████| 18/18 [00:00<00:00, 1124.63it/s]\n",
      "Epoch 66/700: 100%|██████████| 18/18 [00:00<00:00, 723.29it/s]\n",
      "Epoch 67/700: 100%|██████████| 18/18 [00:00<00:00, 1159.57it/s]\n",
      "Epoch 68/700: 100%|██████████| 18/18 [00:00<00:00, 1121.39it/s]\n",
      "Epoch 69/700: 100%|██████████| 18/18 [00:00<00:00, 1092.22it/s]\n",
      "Epoch 70/700: 100%|██████████| 18/18 [00:00<00:00, 764.62it/s]\n",
      "Epoch 71/700: 100%|██████████| 18/18 [00:00<00:00, 1124.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/700], Training Loss: 0.1618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/700: 100%|██████████| 18/18 [00:00<00:00, 749.36it/s]\n",
      "Epoch 73/700: 100%|██████████| 18/18 [00:00<00:00, 1197.54it/s]\n",
      "Epoch 74/700: 100%|██████████| 18/18 [00:00<00:00, 1127.28it/s]\n",
      "Epoch 75/700: 100%|██████████| 18/18 [00:00<00:00, 1095.72it/s]\n",
      "Epoch 76/700: 100%|██████████| 18/18 [00:00<00:00, 546.74it/s]\n",
      "Epoch 77/700: 100%|██████████| 18/18 [00:00<00:00, 755.43it/s]\n",
      "Epoch 78/700: 100%|██████████| 18/18 [00:00<00:00, 748.32it/s]\n",
      "Epoch 79/700: 100%|██████████| 18/18 [00:00<00:00, 751.28it/s]\n",
      "Epoch 80/700: 100%|██████████| 18/18 [00:00<00:00, 746.05it/s]\n",
      "Epoch 81/700: 100%|██████████| 18/18 [00:00<00:00, 1077.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [81/700], Training Loss: 0.1520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/700: 100%|██████████| 18/18 [00:00<00:00, 765.10it/s]\n",
      "Epoch 83/700: 100%|██████████| 18/18 [00:00<00:00, 1123.51it/s]\n",
      "Epoch 84/700: 100%|██████████| 18/18 [00:00<00:00, 764.51it/s]\n",
      "Epoch 85/700: 100%|██████████| 18/18 [00:00<00:00, 1091.46it/s]\n",
      "Epoch 86/700: 100%|██████████| 18/18 [00:00<00:00, 748.97it/s]\n",
      "Epoch 87/700: 100%|██████████| 18/18 [00:00<00:00, 740.58it/s]\n",
      "Epoch 88/700: 100%|██████████| 18/18 [00:00<00:00, 747.43it/s]\n",
      "Epoch 89/700: 100%|██████████| 18/18 [00:00<00:00, 749.06it/s]\n",
      "Epoch 90/700: 100%|██████████| 18/18 [00:00<00:00, 739.46it/s]\n",
      "Epoch 91/700: 100%|██████████| 18/18 [00:00<00:00, 441.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [91/700], Training Loss: 0.1507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/700: 100%|██████████| 18/18 [00:00<00:00, 748.62it/s]\n",
      "Epoch 93/700: 100%|██████████| 18/18 [00:00<00:00, 737.09it/s]\n",
      "Epoch 94/700: 100%|██████████| 18/18 [00:00<00:00, 749.33it/s]\n",
      "Epoch 95/700: 100%|██████████| 18/18 [00:00<00:00, 749.76it/s]\n",
      "Epoch 96/700: 100%|██████████| 18/18 [00:00<00:00, 1433.71it/s]\n",
      "Epoch 97/700: 100%|██████████| 18/18 [00:00<00:00, 732.05it/s]\n",
      "Epoch 98/700: 100%|██████████| 18/18 [00:00<00:00, 1095.36it/s]\n",
      "Epoch 99/700: 100%|██████████| 18/18 [00:00<00:00, 748.59it/s]\n",
      "Epoch 100/700: 100%|██████████| 18/18 [00:00<00:00, 998.76it/s]\n",
      "Epoch 101/700: 100%|██████████| 18/18 [00:00<00:00, 732.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [101/700], Training Loss: 0.1505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 102/700: 100%|██████████| 18/18 [00:00<00:00, 740.22it/s]\n",
      "Epoch 103/700: 100%|██████████| 18/18 [00:00<00:00, 1025.70it/s]\n",
      "Epoch 104/700: 100%|██████████| 18/18 [00:00<00:00, 712.56it/s]\n",
      "Epoch 105/700: 100%|██████████| 18/18 [00:00<00:00, 736.86it/s]\n",
      "Epoch 106/700: 100%|██████████| 18/18 [00:00<00:00, 749.55it/s]\n",
      "Epoch 107/700: 100%|██████████| 18/18 [00:00<00:00, 743.53it/s]\n",
      "Epoch 108/700: 100%|██████████| 18/18 [00:00<00:00, 750.14it/s]\n",
      "Epoch 109/700: 100%|██████████| 18/18 [00:00<00:00, 736.93it/s]\n",
      "Epoch 110/700: 100%|██████████| 18/18 [00:00<00:00, 762.75it/s]\n",
      "Epoch 111/700: 100%|██████████| 18/18 [00:00<00:00, 740.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [111/700], Training Loss: 0.1678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 112/700: 100%|██████████| 18/18 [00:00<00:00, 749.80it/s]\n",
      "Epoch 113/700: 100%|██████████| 18/18 [00:00<00:00, 562.21it/s]\n",
      "Epoch 114/700: 100%|██████████| 18/18 [00:00<00:00, 895.89it/s]\n",
      "Epoch 115/700: 100%|██████████| 18/18 [00:00<00:00, 628.62it/s]\n",
      "Epoch 116/700: 100%|██████████| 18/18 [00:00<00:00, 764.98it/s]\n",
      "Epoch 117/700: 100%|██████████| 18/18 [00:00<00:00, 747.88it/s]\n",
      "Epoch 118/700: 100%|██████████| 18/18 [00:00<00:00, 448.48it/s]\n",
      "Epoch 119/700: 100%|██████████| 18/18 [00:00<00:00, 561.26it/s]\n",
      "Epoch 120/700: 100%|██████████| 18/18 [00:00<00:00, 1118.73it/s]\n",
      "Epoch 121/700: 100%|██████████| 18/18 [00:00<00:00, 1121.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [121/700], Training Loss: 0.1455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 122/700: 100%|██████████| 18/18 [00:00<00:00, 718.58it/s]\n",
      "Epoch 123/700: 100%|██████████| 18/18 [00:00<00:00, 780.70it/s]\n",
      "Epoch 124/700: 100%|██████████| 18/18 [00:00<00:00, 700.52it/s]\n",
      "Epoch 125/700: 100%|██████████| 18/18 [00:00<00:00, 1081.90it/s]\n",
      "Epoch 126/700: 100%|██████████| 18/18 [00:00<00:00, 1114.70it/s]\n",
      "Epoch 127/700: 100%|██████████| 18/18 [00:00<00:00, 1097.79it/s]\n",
      "Epoch 128/700: 100%|██████████| 18/18 [00:00<00:00, 1122.41it/s]\n",
      "Epoch 129/700: 100%|██████████| 18/18 [00:00<00:00, 731.95it/s]\n",
      "Epoch 130/700: 100%|██████████| 18/18 [00:00<00:00, 749.69it/s]\n",
      "Epoch 131/700: 100%|██████████| 18/18 [00:00<00:00, 1122.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [131/700], Training Loss: 0.1461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 132/700: 100%|██████████| 18/18 [00:00<00:00, 749.84it/s]\n",
      "Epoch 133/700: 100%|██████████| 18/18 [00:00<00:00, 737.54it/s]\n",
      "Epoch 134/700: 100%|██████████| 18/18 [00:00<00:00, 704.56it/s]\n",
      "Epoch 135/700: 100%|██████████| 18/18 [00:00<00:00, 1139.81it/s]\n",
      "Epoch 136/700: 100%|██████████| 18/18 [00:00<00:00, 733.39it/s]\n",
      "Epoch 137/700: 100%|██████████| 18/18 [00:00<00:00, 756.81it/s]\n",
      "Epoch 138/700: 100%|██████████| 18/18 [00:00<00:00, 750.21it/s]\n",
      "Epoch 139/700: 100%|██████████| 18/18 [00:00<00:00, 1124.16it/s]\n",
      "Epoch 140/700: 100%|██████████| 18/18 [00:00<00:00, 1106.30it/s]\n",
      "Epoch 141/700: 100%|██████████| 18/18 [00:00<00:00, 745.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [141/700], Training Loss: 0.1433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 142/700: 100%|██████████| 18/18 [00:00<00:00, 555.32it/s]\n",
      "Epoch 143/700: 100%|██████████| 18/18 [00:00<00:00, 726.59it/s]\n",
      "Epoch 144/700: 100%|██████████| 18/18 [00:00<00:00, 377.67it/s]\n",
      "Epoch 145/700: 100%|██████████| 18/18 [00:00<00:00, 747.95it/s]\n",
      "Epoch 146/700: 100%|██████████| 18/18 [00:00<00:00, 1122.39it/s]\n",
      "Epoch 147/700: 100%|██████████| 18/18 [00:00<00:00, 1122.76it/s]\n",
      "Epoch 148/700: 100%|██████████| 18/18 [00:00<00:00, 980.28it/s]\n",
      "Epoch 149/700: 100%|██████████| 18/18 [00:00<00:00, 733.64it/s]\n",
      "Epoch 150/700: 100%|██████████| 18/18 [00:00<00:00, 1124.51it/s]\n",
      "Epoch 151/700: 100%|██████████| 18/18 [00:00<00:00, 1069.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [151/700], Training Loss: 0.1415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 152/700: 100%|██████████| 18/18 [00:00<00:00, 854.75it/s]\n",
      "Epoch 153/700: 100%|██████████| 18/18 [00:00<00:00, 1123.89it/s]\n",
      "Epoch 154/700: 100%|██████████| 18/18 [00:00<00:00, 747.23it/s]\n",
      "Epoch 155/700: 100%|██████████| 18/18 [00:00<00:00, 749.55it/s]\n",
      "Epoch 156/700: 100%|██████████| 18/18 [00:00<00:00, 666.47it/s]\n",
      "Epoch 157/700: 100%|██████████| 18/18 [00:00<00:00, 1024.03it/s]\n",
      "Epoch 158/700: 100%|██████████| 18/18 [00:00<00:00, 736.14it/s]\n",
      "Epoch 159/700: 100%|██████████| 18/18 [00:00<00:00, 736.00it/s]\n",
      "Epoch 160/700: 100%|██████████| 18/18 [00:00<00:00, 561.17it/s]\n",
      "Epoch 161/700: 100%|██████████| 18/18 [00:00<00:00, 447.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [161/700], Training Loss: 0.1552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 162/700: 100%|██████████| 18/18 [00:00<00:00, 556.86it/s]\n",
      "Epoch 163/700: 100%|██████████| 18/18 [00:00<00:00, 559.45it/s]\n",
      "Epoch 164/700: 100%|██████████| 18/18 [00:00<00:00, 743.82it/s]\n",
      "Epoch 165/700: 100%|██████████| 18/18 [00:00<00:00, 721.17it/s]\n",
      "Epoch 166/700: 100%|██████████| 18/18 [00:00<00:00, 730.31it/s]\n",
      "Epoch 167/700: 100%|██████████| 18/18 [00:00<00:00, 1115.23it/s]\n",
      "Epoch 168/700: 100%|██████████| 18/18 [00:00<00:00, 1092.90it/s]\n",
      "Epoch 169/700: 100%|██████████| 18/18 [00:00<00:00, 1123.71it/s]\n",
      "Epoch 170/700: 100%|██████████| 18/18 [00:00<00:00, 748.23it/s]\n",
      "Epoch 171/700: 100%|██████████| 18/18 [00:00<00:00, 963.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [171/700], Training Loss: 0.1457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 172/700: 100%|██████████| 18/18 [00:00<00:00, 1086.14it/s]\n",
      "Epoch 173/700: 100%|██████████| 18/18 [00:00<00:00, 375.01it/s]\n",
      "Epoch 174/700: 100%|██████████| 18/18 [00:00<00:00, 629.32it/s]\n",
      "Epoch 175/700: 100%|██████████| 18/18 [00:00<00:00, 1316.32it/s]\n",
      "Epoch 176/700: 100%|██████████| 18/18 [00:00<00:00, 1123.04it/s]\n",
      "Epoch 177/700: 100%|██████████| 18/18 [00:00<00:00, 1097.97it/s]\n",
      "Epoch 178/700: 100%|██████████| 18/18 [00:00<00:00, 730.88it/s]\n",
      "Epoch 179/700: 100%|██████████| 18/18 [00:00<00:00, 729.09it/s]\n",
      "Epoch 180/700: 100%|██████████| 18/18 [00:00<00:00, 764.38it/s]\n",
      "Epoch 181/700: 100%|██████████| 18/18 [00:00<00:00, 747.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [181/700], Training Loss: 0.1523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 182/700: 100%|██████████| 18/18 [00:00<00:00, 1098.40it/s]\n",
      "Epoch 183/700: 100%|██████████| 18/18 [00:00<00:00, 739.57it/s]\n",
      "Epoch 184/700: 100%|██████████| 18/18 [00:00<00:00, 1102.06it/s]\n",
      "Epoch 185/700: 100%|██████████| 18/18 [00:00<00:00, 1127.25it/s]\n",
      "Epoch 186/700: 100%|██████████| 18/18 [00:00<00:00, 751.32it/s]\n",
      "Epoch 187/700: 100%|██████████| 18/18 [00:00<00:00, 745.22it/s]\n",
      "Epoch 188/700: 100%|██████████| 18/18 [00:00<00:00, 739.87it/s]\n",
      "Epoch 189/700: 100%|██████████| 18/18 [00:00<00:00, 1121.64it/s]\n",
      "Epoch 190/700: 100%|██████████| 18/18 [00:00<00:00, 1124.13it/s]\n",
      "Epoch 191/700: 100%|██████████| 18/18 [00:00<00:00, 1115.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [191/700], Training Loss: 0.1598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 192/700: 100%|██████████| 18/18 [00:00<00:00, 1103.55it/s]\n",
      "Epoch 193/700: 100%|██████████| 18/18 [00:00<00:00, 750.10it/s]\n",
      "Epoch 194/700: 100%|██████████| 18/18 [00:00<00:00, 1124.86it/s]\n",
      "Epoch 195/700: 100%|██████████| 18/18 [00:00<00:00, 742.45it/s]\n",
      "Epoch 196/700: 100%|██████████| 18/18 [00:00<00:00, 721.58it/s]\n",
      "Epoch 197/700: 100%|██████████| 18/18 [00:00<00:00, 579.10it/s]\n",
      "Epoch 198/700: 100%|██████████| 18/18 [00:00<00:00, 1097.03it/s]\n",
      "Epoch 199/700: 100%|██████████| 18/18 [00:00<00:00, 1104.77it/s]\n",
      "Epoch 200/700: 100%|██████████| 18/18 [00:00<00:00, 1152.28it/s]\n",
      "Epoch 201/700: 100%|██████████| 18/18 [00:00<00:00, 764.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [201/700], Training Loss: 0.1434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 202/700: 100%|██████████| 18/18 [00:00<00:00, 544.58it/s]\n",
      "Epoch 203/700: 100%|██████████| 18/18 [00:00<00:00, 1124.75it/s]\n",
      "Epoch 204/700: 100%|██████████| 18/18 [00:00<00:00, 740.36it/s]\n",
      "Epoch 205/700: 100%|██████████| 18/18 [00:00<00:00, 736.40it/s]\n",
      "Epoch 206/700: 100%|██████████| 18/18 [00:00<00:00, 551.95it/s]\n",
      "Epoch 207/700: 100%|██████████| 18/18 [00:00<00:00, 556.21it/s]\n",
      "Epoch 208/700: 100%|██████████| 18/18 [00:00<00:00, 743.83it/s]\n",
      "Epoch 209/700: 100%|██████████| 18/18 [00:00<00:00, 745.88it/s]\n",
      "Epoch 210/700: 100%|██████████| 18/18 [00:00<00:00, 745.46it/s]\n",
      "Epoch 211/700: 100%|██████████| 18/18 [00:00<00:00, 748.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [211/700], Training Loss: 0.1456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 212/700: 100%|██████████| 18/18 [00:00<00:00, 736.60it/s]\n",
      "Epoch 213/700: 100%|██████████| 18/18 [00:00<00:00, 741.62it/s]\n",
      "Epoch 214/700: 100%|██████████| 18/18 [00:00<00:00, 1126.00it/s]\n",
      "Epoch 215/700: 100%|██████████| 18/18 [00:00<00:00, 867.28it/s]\n",
      "Epoch 216/700: 100%|██████████| 18/18 [00:00<00:00, 760.81it/s]\n",
      "Epoch 217/700: 100%|██████████| 18/18 [00:00<00:00, 870.55it/s]\n",
      "Epoch 218/700: 100%|██████████| 18/18 [00:00<00:00, 748.01it/s]\n",
      "Epoch 219/700: 100%|██████████| 18/18 [00:00<00:00, 1127.18it/s]\n",
      "Epoch 220/700: 100%|██████████| 18/18 [00:00<00:00, 1123.81it/s]\n",
      "Epoch 221/700: 100%|██████████| 18/18 [00:00<00:00, 1077.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [221/700], Training Loss: 0.1416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 222/700: 100%|██████████| 18/18 [00:00<00:00, 748.77it/s]\n",
      "Epoch 223/700: 100%|██████████| 18/18 [00:00<00:00, 1123.16it/s]\n",
      "Epoch 224/700: 100%|██████████| 18/18 [00:00<00:00, 1113.56it/s]\n",
      "Epoch 225/700: 100%|██████████| 18/18 [00:00<00:00, 1147.40it/s]\n",
      "Epoch 226/700: 100%|██████████| 18/18 [00:00<00:00, 749.37it/s]\n",
      "Epoch 227/700: 100%|██████████| 18/18 [00:00<00:00, 750.06it/s]\n",
      "Epoch 228/700: 100%|██████████| 18/18 [00:00<00:00, 1122.89it/s]\n",
      "Epoch 229/700: 100%|██████████| 18/18 [00:00<00:00, 1109.80it/s]\n",
      "Epoch 230/700: 100%|██████████| 18/18 [00:00<00:00, 455.06it/s]\n",
      "Epoch 231/700: 100%|██████████| 18/18 [00:00<00:00, 741.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [231/700], Training Loss: 0.1468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 232/700: 100%|██████████| 18/18 [00:00<00:00, 1021.49it/s]\n",
      "Epoch 233/700: 100%|██████████| 18/18 [00:00<00:00, 780.71it/s]\n",
      "Epoch 234/700: 100%|██████████| 18/18 [00:00<00:00, 1098.59it/s]\n",
      "Epoch 235/700: 100%|██████████| 18/18 [00:00<00:00, 734.03it/s]\n",
      "Epoch 236/700: 100%|██████████| 18/18 [00:00<00:00, 724.94it/s]\n",
      "Epoch 237/700: 100%|██████████| 18/18 [00:00<00:00, 916.78it/s]\n",
      "Epoch 238/700: 100%|██████████| 18/18 [00:00<00:00, 716.65it/s]\n",
      "Epoch 239/700: 100%|██████████| 18/18 [00:00<00:00, 1080.73it/s]\n",
      "Epoch 240/700: 100%|██████████| 18/18 [00:00<00:00, 754.27it/s]\n",
      "Epoch 241/700: 100%|██████████| 18/18 [00:00<00:00, 1054.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [241/700], Training Loss: 0.1380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 242/700: 100%|██████████| 18/18 [00:00<00:00, 545.61it/s]\n",
      "Epoch 243/700: 100%|██████████| 18/18 [00:00<00:00, 1107.03it/s]\n",
      "Epoch 244/700: 100%|██████████| 18/18 [00:00<00:00, 1120.32it/s]\n",
      "Epoch 245/700: 100%|██████████| 18/18 [00:00<00:00, 746.69it/s]\n",
      "Epoch 246/700: 100%|██████████| 18/18 [00:00<00:00, 749.11it/s]\n",
      "Epoch 247/700: 100%|██████████| 18/18 [00:00<00:00, 1158.27it/s]\n",
      "Epoch 248/700: 100%|██████████| 18/18 [00:00<00:00, 1124.11it/s]\n",
      "Epoch 249/700: 100%|██████████| 18/18 [00:00<00:00, 741.82it/s]\n",
      "Epoch 250/700: 100%|██████████| 18/18 [00:00<00:00, 747.77it/s]\n",
      "Epoch 251/700: 100%|██████████| 18/18 [00:00<00:00, 750.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [251/700], Training Loss: 0.1795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 252/700: 100%|██████████| 18/18 [00:00<00:00, 709.18it/s]\n",
      "Epoch 253/700: 100%|██████████| 18/18 [00:00<00:00, 1118.08it/s]\n",
      "Epoch 254/700: 100%|██████████| 18/18 [00:00<00:00, 1118.91it/s]\n",
      "Epoch 255/700: 100%|██████████| 18/18 [00:00<00:00, 751.30it/s]\n",
      "Epoch 256/700: 100%|██████████| 18/18 [00:00<00:00, 739.29it/s]\n",
      "Epoch 257/700: 100%|██████████| 18/18 [00:00<00:00, 403.39it/s]\n",
      "Epoch 258/700: 100%|██████████| 18/18 [00:00<00:00, 574.95it/s]\n",
      "Epoch 259/700: 100%|██████████| 18/18 [00:00<00:00, 741.09it/s]\n",
      "Epoch 260/700: 100%|██████████| 18/18 [00:00<00:00, 1122.11it/s]\n",
      "Epoch 261/700: 100%|██████████| 18/18 [00:00<00:00, 1123.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [261/700], Training Loss: 0.1363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 262/700: 100%|██████████| 18/18 [00:00<00:00, 750.31it/s]\n",
      "Epoch 263/700: 100%|██████████| 18/18 [00:00<00:00, 749.88it/s]\n",
      "Epoch 264/700: 100%|██████████| 18/18 [00:00<00:00, 873.62it/s]\n",
      "Epoch 265/700: 100%|██████████| 18/18 [00:00<00:00, 749.15it/s]\n",
      "Epoch 266/700: 100%|██████████| 18/18 [00:00<00:00, 750.77it/s]\n",
      "Epoch 267/700: 100%|██████████| 18/18 [00:00<00:00, 555.92it/s]\n",
      "Epoch 268/700: 100%|██████████| 18/18 [00:00<00:00, 756.85it/s]\n",
      "Epoch 269/700: 100%|██████████| 18/18 [00:00<00:00, 1126.39it/s]\n",
      "Epoch 270/700: 100%|██████████| 18/18 [00:00<00:00, 1146.16it/s]\n",
      "Epoch 271/700: 100%|██████████| 18/18 [00:00<00:00, 746.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [271/700], Training Loss: 0.1392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 272/700: 100%|██████████| 18/18 [00:00<00:00, 742.13it/s]\n",
      "Epoch 273/700: 100%|██████████| 18/18 [00:00<00:00, 712.06it/s]\n",
      "Epoch 274/700: 100%|██████████| 18/18 [00:00<00:00, 741.72it/s]\n",
      "Epoch 275/700: 100%|██████████| 18/18 [00:00<00:00, 796.34it/s]\n",
      "Epoch 276/700: 100%|██████████| 18/18 [00:00<00:00, 1123.61it/s]\n",
      "Epoch 277/700: 100%|██████████| 18/18 [00:00<00:00, 829.66it/s]\n",
      "Epoch 278/700: 100%|██████████| 18/18 [00:00<00:00, 1092.05it/s]\n",
      "Epoch 279/700: 100%|██████████| 18/18 [00:00<00:00, 1127.92it/s]\n",
      "Epoch 280/700: 100%|██████████| 18/18 [00:00<00:00, 745.83it/s]\n",
      "Epoch 281/700: 100%|██████████| 18/18 [00:00<00:00, 729.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [281/700], Training Loss: 0.1343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 282/700: 100%|██████████| 18/18 [00:00<00:00, 1104.86it/s]\n",
      "Epoch 283/700: 100%|██████████| 18/18 [00:00<00:00, 747.20it/s]\n",
      "Epoch 284/700: 100%|██████████| 18/18 [00:00<00:00, 1069.48it/s]\n",
      "Epoch 285/700: 100%|██████████| 18/18 [00:00<00:00, 748.17it/s]\n",
      "Epoch 286/700: 100%|██████████| 18/18 [00:00<00:00, 750.16it/s]\n",
      "Epoch 287/700: 100%|██████████| 18/18 [00:00<00:00, 1052.83it/s]\n",
      "Epoch 288/700: 100%|██████████| 18/18 [00:00<00:00, 1089.63it/s]\n",
      "Epoch 289/700: 100%|██████████| 18/18 [00:00<00:00, 747.72it/s]\n",
      "Epoch 290/700: 100%|██████████| 18/18 [00:00<00:00, 1110.45it/s]\n",
      "Epoch 291/700: 100%|██████████| 18/18 [00:00<00:00, 744.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [291/700], Training Loss: 0.1384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 292/700: 100%|██████████| 18/18 [00:00<00:00, 943.75it/s]\n",
      "Epoch 293/700: 100%|██████████| 18/18 [00:00<00:00, 1123.34it/s]\n",
      "Epoch 294/700: 100%|██████████| 18/18 [00:00<00:00, 1124.66it/s]\n",
      "Epoch 295/700: 100%|██████████| 18/18 [00:00<00:00, 1127.91it/s]\n",
      "Epoch 296/700: 100%|██████████| 18/18 [00:00<00:00, 750.09it/s]\n",
      "Epoch 297/700: 100%|██████████| 18/18 [00:00<00:00, 1078.10it/s]\n",
      "Epoch 298/700: 100%|██████████| 18/18 [00:00<00:00, 1098.03it/s]\n",
      "Epoch 299/700: 100%|██████████| 18/18 [00:00<00:00, 749.59it/s]\n",
      "Epoch 300/700: 100%|██████████| 18/18 [00:00<00:00, 1519.19it/s]\n",
      "Epoch 301/700: 100%|██████████| 18/18 [00:00<00:00, 1122.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [301/700], Training Loss: 0.1517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 302/700: 100%|██████████| 18/18 [00:00<00:00, 1094.64it/s]\n",
      "Epoch 303/700: 100%|██████████| 18/18 [00:00<00:00, 749.41it/s]\n",
      "Epoch 304/700: 100%|██████████| 18/18 [00:00<00:00, 1124.44it/s]\n",
      "Epoch 305/700: 100%|██████████| 18/18 [00:00<00:00, 1122.04it/s]\n",
      "Epoch 306/700: 100%|██████████| 18/18 [00:00<00:00, 1156.38it/s]\n",
      "Epoch 307/700: 100%|██████████| 18/18 [00:00<00:00, 705.43it/s]\n",
      "Epoch 308/700: 100%|██████████| 18/18 [00:00<00:00, 1120.66it/s]\n",
      "Epoch 309/700: 100%|██████████| 18/18 [00:00<00:00, 749.58it/s]\n",
      "Epoch 310/700: 100%|██████████| 18/18 [00:00<00:00, 1114.85it/s]\n",
      "Epoch 311/700: 100%|██████████| 18/18 [00:00<00:00, 1115.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [311/700], Training Loss: 0.1341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 312/700: 100%|██████████| 18/18 [00:00<00:00, 443.73it/s]\n",
      "Epoch 313/700: 100%|██████████| 18/18 [00:00<00:00, 413.32it/s]\n",
      "Epoch 314/700: 100%|██████████| 18/18 [00:00<00:00, 749.82it/s]\n",
      "Epoch 315/700: 100%|██████████| 18/18 [00:00<00:00, 747.95it/s]\n",
      "Epoch 316/700: 100%|██████████| 18/18 [00:00<00:00, 751.65it/s]\n",
      "Epoch 317/700: 100%|██████████| 18/18 [00:00<00:00, 760.36it/s]\n",
      "Epoch 318/700: 100%|██████████| 18/18 [00:00<00:00, 751.76it/s]\n",
      "Epoch 319/700: 100%|██████████| 18/18 [00:00<00:00, 502.90it/s]\n",
      "Epoch 320/700: 100%|██████████| 18/18 [00:00<00:00, 1038.78it/s]\n",
      "Epoch 321/700: 100%|██████████| 18/18 [00:00<00:00, 745.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [321/700], Training Loss: 0.1380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 322/700: 100%|██████████| 18/18 [00:00<00:00, 559.08it/s]\n",
      "Epoch 323/700: 100%|██████████| 18/18 [00:00<00:00, 1087.99it/s]\n",
      "Epoch 324/700: 100%|██████████| 18/18 [00:00<00:00, 1092.90it/s]\n",
      "Epoch 325/700: 100%|██████████| 18/18 [00:00<00:00, 1119.68it/s]\n",
      "Epoch 326/700: 100%|██████████| 18/18 [00:00<00:00, 1125.60it/s]\n",
      "Epoch 327/700: 100%|██████████| 18/18 [00:00<00:00, 749.60it/s]\n",
      "Epoch 328/700: 100%|██████████| 18/18 [00:00<00:00, 737.24it/s]\n",
      "Epoch 329/700: 100%|██████████| 18/18 [00:00<00:00, 733.70it/s]\n",
      "Epoch 330/700: 100%|██████████| 18/18 [00:00<00:00, 750.24it/s]\n",
      "Epoch 331/700: 100%|██████████| 18/18 [00:00<00:00, 746.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [331/700], Training Loss: 0.1443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 332/700: 100%|██████████| 18/18 [00:00<00:00, 1133.19it/s]\n",
      "Epoch 333/700: 100%|██████████| 18/18 [00:00<00:00, 1056.97it/s]\n",
      "Epoch 334/700: 100%|██████████| 18/18 [00:00<00:00, 761.40it/s]\n",
      "Epoch 335/700: 100%|██████████| 18/18 [00:00<00:00, 1076.40it/s]\n",
      "Epoch 336/700: 100%|██████████| 18/18 [00:00<00:00, 1123.31it/s]\n",
      "Epoch 337/700: 100%|██████████| 18/18 [00:00<00:00, 740.45it/s]\n",
      "Epoch 338/700: 100%|██████████| 18/18 [00:00<00:00, 744.10it/s]\n",
      "Epoch 339/700: 100%|██████████| 18/18 [00:00<00:00, 1093.01it/s]\n",
      "Epoch 340/700: 100%|██████████| 18/18 [00:00<00:00, 432.28it/s]\n",
      "Epoch 341/700: 100%|██████████| 18/18 [00:00<00:00, 741.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [341/700], Training Loss: 0.1333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 342/700: 100%|██████████| 18/18 [00:00<00:00, 748.33it/s]\n",
      "Epoch 343/700: 100%|██████████| 18/18 [00:00<00:00, 1119.94it/s]\n",
      "Epoch 344/700: 100%|██████████| 18/18 [00:00<00:00, 1155.67it/s]\n",
      "Epoch 345/700: 100%|██████████| 18/18 [00:00<00:00, 725.72it/s]\n",
      "Epoch 346/700: 100%|██████████| 18/18 [00:00<00:00, 1121.54it/s]\n",
      "Epoch 347/700: 100%|██████████| 18/18 [00:00<00:00, 1129.29it/s]\n",
      "Epoch 348/700: 100%|██████████| 18/18 [00:00<00:00, 749.64it/s]\n",
      "Epoch 349/700: 100%|██████████| 18/18 [00:00<00:00, 742.27it/s]\n",
      "Epoch 350/700: 100%|██████████| 18/18 [00:00<00:00, 1120.17it/s]\n",
      "Epoch 351/700: 100%|██████████| 18/18 [00:00<00:00, 1123.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [351/700], Training Loss: 0.1399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 352/700: 100%|██████████| 18/18 [00:00<00:00, 1096.84it/s]\n",
      "Epoch 353/700: 100%|██████████| 18/18 [00:00<00:00, 749.67it/s]\n",
      "Epoch 354/700: 100%|██████████| 18/18 [00:00<00:00, 747.70it/s]\n",
      "Epoch 355/700: 100%|██████████| 18/18 [00:00<00:00, 1485.62it/s]\n",
      "Epoch 356/700: 100%|██████████| 18/18 [00:00<00:00, 733.32it/s]\n",
      "Epoch 357/700: 100%|██████████| 18/18 [00:00<00:00, 750.32it/s]\n",
      "Epoch 358/700: 100%|██████████| 18/18 [00:00<00:00, 1126.49it/s]\n",
      "Epoch 359/700: 100%|██████████| 18/18 [00:00<00:00, 1123.04it/s]\n",
      "Epoch 360/700: 100%|██████████| 18/18 [00:00<00:00, 747.72it/s]\n",
      "Epoch 361/700: 100%|██████████| 18/18 [00:00<00:00, 1095.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [361/700], Training Loss: 0.1397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 362/700: 100%|██████████| 18/18 [00:00<00:00, 1122.11it/s]\n",
      "Epoch 363/700: 100%|██████████| 18/18 [00:00<00:00, 749.52it/s]\n",
      "Epoch 364/700: 100%|██████████| 18/18 [00:00<00:00, 827.96it/s]\n",
      "Epoch 365/700: 100%|██████████| 18/18 [00:00<00:00, 1154.94it/s]\n",
      "Epoch 366/700: 100%|██████████| 18/18 [00:00<00:00, 1121.47it/s]\n",
      "Epoch 367/700: 100%|██████████| 18/18 [00:00<00:00, 372.35it/s]\n",
      "Epoch 368/700: 100%|██████████| 18/18 [00:00<00:00, 556.80it/s]\n",
      "Epoch 369/700: 100%|██████████| 18/18 [00:00<00:00, 751.33it/s]\n",
      "Epoch 370/700: 100%|██████████| 18/18 [00:00<00:00, 744.74it/s]\n",
      "Epoch 371/700: 100%|██████████| 18/18 [00:00<00:00, 747.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [371/700], Training Loss: 0.1332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 372/700: 100%|██████████| 18/18 [00:00<00:00, 744.30it/s]\n",
      "Epoch 373/700: 100%|██████████| 18/18 [00:00<00:00, 750.91it/s]\n",
      "Epoch 374/700: 100%|██████████| 18/18 [00:00<00:00, 747.48it/s]\n",
      "Epoch 375/700: 100%|██████████| 18/18 [00:00<00:00, 1124.39it/s]\n",
      "Epoch 376/700: 100%|██████████| 18/18 [00:00<00:00, 1128.60it/s]\n",
      "Epoch 377/700: 100%|██████████| 18/18 [00:00<00:00, 749.31it/s]\n",
      "Epoch 378/700: 100%|██████████| 18/18 [00:00<00:00, 744.93it/s]\n",
      "Epoch 379/700: 100%|██████████| 18/18 [00:00<00:00, 554.71it/s]\n",
      "Epoch 380/700: 100%|██████████| 18/18 [00:00<00:00, 747.78it/s]\n",
      "Epoch 381/700: 100%|██████████| 18/18 [00:00<00:00, 746.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [381/700], Training Loss: 0.1395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 382/700: 100%|██████████| 18/18 [00:00<00:00, 744.45it/s]\n",
      "Epoch 383/700: 100%|██████████| 18/18 [00:00<00:00, 673.59it/s]\n",
      "Epoch 384/700: 100%|██████████| 18/18 [00:00<00:00, 691.15it/s]\n",
      "Epoch 385/700: 100%|██████████| 18/18 [00:00<00:00, 792.28it/s]\n",
      "Epoch 386/700: 100%|██████████| 18/18 [00:00<00:00, 734.73it/s]\n",
      "Epoch 387/700: 100%|██████████| 18/18 [00:00<00:00, 735.94it/s]\n",
      "Epoch 388/700: 100%|██████████| 18/18 [00:00<00:00, 706.07it/s]\n",
      "Epoch 389/700: 100%|██████████| 18/18 [00:00<00:00, 1120.07it/s]\n",
      "Epoch 390/700: 100%|██████████| 18/18 [00:00<00:00, 746.59it/s]\n",
      "Epoch 391/700: 100%|██████████| 18/18 [00:00<00:00, 744.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [391/700], Training Loss: 0.1339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 392/700: 100%|██████████| 18/18 [00:00<00:00, 563.25it/s]\n",
      "Epoch 393/700: 100%|██████████| 18/18 [00:00<00:00, 744.25it/s]\n",
      "Epoch 394/700: 100%|██████████| 18/18 [00:00<00:00, 447.06it/s]\n",
      "Epoch 395/700: 100%|██████████| 18/18 [00:00<00:00, 737.73it/s]\n",
      "Epoch 396/700: 100%|██████████| 18/18 [00:00<00:00, 744.04it/s]\n",
      "Epoch 397/700: 100%|██████████| 18/18 [00:00<00:00, 1125.13it/s]\n",
      "Epoch 398/700: 100%|██████████| 18/18 [00:00<00:00, 738.84it/s]\n",
      "Epoch 399/700: 100%|██████████| 18/18 [00:00<00:00, 1078.49it/s]\n",
      "Epoch 400/700: 100%|██████████| 18/18 [00:00<00:00, 1118.66it/s]\n",
      "Epoch 401/700: 100%|██████████| 18/18 [00:00<00:00, 1125.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [401/700], Training Loss: 0.1331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 402/700: 100%|██████████| 18/18 [00:00<00:00, 1115.80it/s]\n",
      "Epoch 403/700: 100%|██████████| 18/18 [00:00<00:00, 1195.64it/s]\n",
      "Epoch 404/700: 100%|██████████| 18/18 [00:00<00:00, 751.23it/s]\n",
      "Epoch 405/700: 100%|██████████| 18/18 [00:00<00:00, 883.83it/s]\n",
      "Epoch 406/700: 100%|██████████| 18/18 [00:00<00:00, 738.66it/s]\n",
      "Epoch 407/700: 100%|██████████| 18/18 [00:00<00:00, 750.25it/s]\n",
      "Epoch 408/700: 100%|██████████| 18/18 [00:00<00:00, 748.43it/s]\n",
      "Epoch 409/700: 100%|██████████| 18/18 [00:00<00:00, 1091.16it/s]\n",
      "Epoch 410/700: 100%|██████████| 18/18 [00:00<00:00, 736.18it/s]\n",
      "Epoch 411/700: 100%|██████████| 18/18 [00:00<00:00, 1098.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [411/700], Training Loss: 0.1364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 412/700: 100%|██████████| 18/18 [00:00<00:00, 746.63it/s]\n",
      "Epoch 413/700: 100%|██████████| 18/18 [00:00<00:00, 735.52it/s]\n",
      "Epoch 414/700: 100%|██████████| 18/18 [00:00<00:00, 1122.87it/s]\n",
      "Epoch 415/700: 100%|██████████| 18/18 [00:00<00:00, 735.29it/s]\n",
      "Epoch 416/700: 100%|██████████| 18/18 [00:00<00:00, 718.44it/s]\n",
      "Epoch 417/700: 100%|██████████| 18/18 [00:00<00:00, 741.43it/s]\n",
      "Epoch 418/700: 100%|██████████| 18/18 [00:00<00:00, 722.35it/s]\n",
      "Epoch 419/700: 100%|██████████| 18/18 [00:00<00:00, 1016.34it/s]\n",
      "Epoch 420/700: 100%|██████████| 18/18 [00:00<00:00, 746.66it/s]\n",
      "Epoch 421/700: 100%|██████████| 18/18 [00:00<00:00, 733.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [421/700], Training Loss: 0.1349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 422/700: 100%|██████████| 18/18 [00:00<00:00, 320.64it/s]\n",
      "Epoch 423/700: 100%|██████████| 18/18 [00:00<00:00, 745.65it/s]\n",
      "Epoch 424/700: 100%|██████████| 18/18 [00:00<00:00, 1124.21it/s]\n",
      "Epoch 425/700: 100%|██████████| 18/18 [00:00<00:00, 1079.77it/s]\n",
      "Epoch 426/700: 100%|██████████| 18/18 [00:00<00:00, 1113.04it/s]\n",
      "Epoch 427/700: 100%|██████████| 18/18 [00:00<00:00, 1107.18it/s]\n",
      "Epoch 428/700: 100%|██████████| 18/18 [00:00<00:00, 749.88it/s]\n",
      "Epoch 429/700: 100%|██████████| 18/18 [00:00<00:00, 1080.45it/s]\n",
      "Epoch 430/700: 100%|██████████| 18/18 [00:00<00:00, 747.57it/s]\n",
      "Epoch 431/700: 100%|██████████| 18/18 [00:00<00:00, 1125.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [431/700], Training Loss: 0.1367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 432/700: 100%|██████████| 18/18 [00:00<00:00, 1115.98it/s]\n",
      "Epoch 433/700: 100%|██████████| 18/18 [00:00<00:00, 558.80it/s]\n",
      "Epoch 434/700: 100%|██████████| 18/18 [00:00<00:00, 1156.15it/s]\n",
      "Epoch 435/700: 100%|██████████| 18/18 [00:00<00:00, 756.61it/s]\n",
      "Epoch 436/700: 100%|██████████| 18/18 [00:00<00:00, 738.76it/s]\n",
      "Epoch 437/700: 100%|██████████| 18/18 [00:00<00:00, 877.71it/s]\n",
      "Epoch 438/700: 100%|██████████| 18/18 [00:00<00:00, 744.40it/s]\n",
      "Epoch 439/700: 100%|██████████| 18/18 [00:00<00:00, 746.95it/s]\n",
      "Epoch 440/700: 100%|██████████| 18/18 [00:00<00:00, 728.47it/s]\n",
      "Epoch 441/700: 100%|██████████| 18/18 [00:00<00:00, 1082.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [441/700], Training Loss: 0.1368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 442/700: 100%|██████████| 18/18 [00:00<00:00, 1123.19it/s]\n",
      "Epoch 443/700: 100%|██████████| 18/18 [00:00<00:00, 552.98it/s]\n",
      "Epoch 444/700: 100%|██████████| 18/18 [00:00<00:00, 1125.05it/s]\n",
      "Epoch 445/700: 100%|██████████| 18/18 [00:00<00:00, 745.46it/s]\n",
      "Epoch 446/700: 100%|██████████| 18/18 [00:00<00:00, 735.47it/s]\n",
      "Epoch 447/700: 100%|██████████| 18/18 [00:00<00:00, 1194.07it/s]\n",
      "Epoch 448/700: 100%|██████████| 18/18 [00:00<00:00, 1194.15it/s]\n",
      "Epoch 449/700: 100%|██████████| 18/18 [00:00<00:00, 747.34it/s]\n",
      "Epoch 450/700: 100%|██████████| 18/18 [00:00<00:00, 1072.82it/s]\n",
      "Epoch 451/700: 100%|██████████| 18/18 [00:00<00:00, 548.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [451/700], Training Loss: 0.1344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 452/700: 100%|██████████| 18/18 [00:00<00:00, 527.48it/s]\n",
      "Epoch 453/700: 100%|██████████| 18/18 [00:00<00:00, 1120.41it/s]\n",
      "Epoch 454/700: 100%|██████████| 18/18 [00:00<00:00, 1127.72it/s]\n",
      "Epoch 455/700: 100%|██████████| 18/18 [00:00<00:00, 1093.48it/s]\n",
      "Epoch 456/700: 100%|██████████| 18/18 [00:00<00:00, 1111.25it/s]\n",
      "Epoch 457/700: 100%|██████████| 18/18 [00:00<00:00, 741.04it/s]\n",
      "Epoch 458/700: 100%|██████████| 18/18 [00:00<00:00, 737.18it/s]\n",
      "Epoch 459/700: 100%|██████████| 18/18 [00:00<00:00, 748.89it/s]\n",
      "Epoch 460/700: 100%|██████████| 18/18 [00:00<00:00, 1098.67it/s]\n",
      "Epoch 461/700: 100%|██████████| 18/18 [00:00<00:00, 1085.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [461/700], Training Loss: 0.1384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 462/700: 100%|██████████| 18/18 [00:00<00:00, 734.56it/s]\n",
      "Epoch 463/700: 100%|██████████| 18/18 [00:00<00:00, 617.24it/s]\n",
      "Epoch 464/700: 100%|██████████| 18/18 [00:00<00:00, 1057.18it/s]\n",
      "Epoch 465/700: 100%|██████████| 18/18 [00:00<00:00, 946.83it/s]\n",
      "Epoch 466/700: 100%|██████████| 18/18 [00:00<00:00, 854.37it/s]\n",
      "Epoch 467/700: 100%|██████████| 18/18 [00:00<00:00, 749.06it/s]\n",
      "Epoch 468/700: 100%|██████████| 18/18 [00:00<00:00, 1115.69it/s]\n",
      "Epoch 469/700: 100%|██████████| 18/18 [00:00<00:00, 696.27it/s]\n",
      "Epoch 470/700: 100%|██████████| 18/18 [00:00<00:00, 781.25it/s]\n",
      "Epoch 471/700: 100%|██████████| 18/18 [00:00<00:00, 739.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [471/700], Training Loss: 0.1523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 472/700: 100%|██████████| 18/18 [00:00<00:00, 738.97it/s]\n",
      "Epoch 473/700: 100%|██████████| 18/18 [00:00<00:00, 744.81it/s]\n",
      "Epoch 474/700: 100%|██████████| 18/18 [00:00<00:00, 554.97it/s]\n",
      "Epoch 475/700: 100%|██████████| 18/18 [00:00<00:00, 744.93it/s]\n",
      "Epoch 476/700: 100%|██████████| 18/18 [00:00<00:00, 747.00it/s]\n",
      "Epoch 477/700: 100%|██████████| 18/18 [00:00<00:00, 737.95it/s]\n",
      "Epoch 478/700: 100%|██████████| 18/18 [00:00<00:00, 562.07it/s]\n",
      "Epoch 479/700: 100%|██████████| 18/18 [00:00<00:00, 447.43it/s]\n",
      "Epoch 480/700: 100%|██████████| 18/18 [00:00<00:00, 736.81it/s]\n",
      "Epoch 481/700: 100%|██████████| 18/18 [00:00<00:00, 1061.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [481/700], Training Loss: 0.1365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 482/700: 100%|██████████| 18/18 [00:00<00:00, 1109.26it/s]\n",
      "Epoch 483/700: 100%|██████████| 18/18 [00:00<00:00, 714.95it/s]\n",
      "Epoch 484/700: 100%|██████████| 18/18 [00:00<00:00, 1027.06it/s]\n",
      "Epoch 485/700: 100%|██████████| 18/18 [00:00<00:00, 1124.83it/s]\n",
      "Epoch 486/700: 100%|██████████| 18/18 [00:00<00:00, 1126.98it/s]\n",
      "Epoch 487/700: 100%|██████████| 18/18 [00:00<00:00, 747.77it/s]\n",
      "Epoch 488/700: 100%|██████████| 18/18 [00:00<00:00, 1116.56it/s]\n",
      "Epoch 489/700: 100%|██████████| 18/18 [00:00<00:00, 735.25it/s]\n",
      "Epoch 490/700: 100%|██████████| 18/18 [00:00<00:00, 1116.23it/s]\n",
      "Epoch 491/700: 100%|██████████| 18/18 [00:00<00:00, 749.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [491/700], Training Loss: 0.1317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 492/700: 100%|██████████| 18/18 [00:00<00:00, 1047.05it/s]\n",
      "Epoch 493/700: 100%|██████████| 18/18 [00:00<00:00, 748.39it/s]\n",
      "Epoch 494/700: 100%|██████████| 18/18 [00:00<00:00, 1099.84it/s]\n",
      "Epoch 495/700: 100%|██████████| 18/18 [00:00<00:00, 749.41it/s]\n",
      "Epoch 496/700: 100%|██████████| 18/18 [00:00<00:00, 1125.00it/s]\n",
      "Epoch 497/700: 100%|██████████| 18/18 [00:00<00:00, 1118.81it/s]\n",
      "Epoch 498/700: 100%|██████████| 18/18 [00:00<00:00, 750.38it/s]\n",
      "Epoch 499/700: 100%|██████████| 18/18 [00:00<00:00, 1089.70it/s]\n",
      "Epoch 500/700: 100%|██████████| 18/18 [00:00<00:00, 764.28it/s]\n",
      "Epoch 501/700: 100%|██████████| 18/18 [00:00<00:00, 736.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [501/700], Training Loss: 0.1316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 502/700: 100%|██████████| 18/18 [00:00<00:00, 747.50it/s]\n",
      "Epoch 503/700: 100%|██████████| 18/18 [00:00<00:00, 734.74it/s]\n",
      "Epoch 504/700: 100%|██████████| 18/18 [00:00<00:00, 1122.19it/s]\n",
      "Epoch 505/700: 100%|██████████| 18/18 [00:00<00:00, 748.20it/s]\n",
      "Epoch 506/700: 100%|██████████| 18/18 [00:00<00:00, 448.49it/s]\n",
      "Epoch 507/700: 100%|██████████| 18/18 [00:00<00:00, 764.24it/s]\n",
      "Epoch 508/700: 100%|██████████| 18/18 [00:00<00:00, 812.67it/s]\n",
      "Epoch 509/700: 100%|██████████| 18/18 [00:00<00:00, 1121.65it/s]\n",
      "Epoch 510/700: 100%|██████████| 18/18 [00:00<00:00, 750.70it/s]\n",
      "Epoch 511/700: 100%|██████████| 18/18 [00:00<00:00, 737.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [511/700], Training Loss: 0.1329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 512/700: 100%|██████████| 18/18 [00:00<00:00, 1126.46it/s]\n",
      "Epoch 513/700: 100%|██████████| 18/18 [00:00<00:00, 557.19it/s]\n",
      "Epoch 514/700: 100%|██████████| 18/18 [00:00<00:00, 750.35it/s]\n",
      "Epoch 515/700: 100%|██████████| 18/18 [00:00<00:00, 735.23it/s]\n",
      "Epoch 516/700: 100%|██████████| 18/18 [00:00<00:00, 746.71it/s]\n",
      "Epoch 517/700: 100%|██████████| 18/18 [00:00<00:00, 945.60it/s]\n",
      "Epoch 518/700: 100%|██████████| 18/18 [00:00<00:00, 562.53it/s]\n",
      "Epoch 519/700: 100%|██████████| 18/18 [00:00<00:00, 559.96it/s]\n",
      "Epoch 520/700: 100%|██████████| 18/18 [00:00<00:00, 557.47it/s]\n",
      "Epoch 521/700: 100%|██████████| 18/18 [00:00<00:00, 752.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [521/700], Training Loss: 0.1345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 522/700: 100%|██████████| 18/18 [00:00<00:00, 1102.38it/s]\n",
      "Epoch 523/700: 100%|██████████| 18/18 [00:00<00:00, 550.64it/s]\n",
      "Epoch 524/700: 100%|██████████| 18/18 [00:00<00:00, 749.41it/s]\n",
      "Epoch 525/700: 100%|██████████| 18/18 [00:00<00:00, 726.49it/s]\n",
      "Epoch 526/700: 100%|██████████| 18/18 [00:00<00:00, 1028.30it/s]\n",
      "Epoch 527/700: 100%|██████████| 18/18 [00:00<00:00, 1133.78it/s]\n",
      "Epoch 528/700: 100%|██████████| 18/18 [00:00<00:00, 742.21it/s]\n",
      "Epoch 529/700: 100%|██████████| 18/18 [00:00<00:00, 751.93it/s]\n",
      "Epoch 530/700: 100%|██████████| 18/18 [00:00<00:00, 742.57it/s]\n",
      "Epoch 531/700: 100%|██████████| 18/18 [00:00<00:00, 918.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [531/700], Training Loss: 0.1326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 532/700: 100%|██████████| 18/18 [00:00<00:00, 1096.68it/s]\n",
      "Epoch 533/700: 100%|██████████| 18/18 [00:00<00:00, 747.01it/s]\n",
      "Epoch 534/700: 100%|██████████| 18/18 [00:00<00:00, 449.20it/s]\n",
      "Epoch 535/700: 100%|██████████| 18/18 [00:00<00:00, 1116.40it/s]\n",
      "Epoch 536/700: 100%|██████████| 18/18 [00:00<00:00, 1116.96it/s]\n",
      "Epoch 537/700: 100%|██████████| 18/18 [00:00<00:00, 751.44it/s]\n",
      "Epoch 538/700: 100%|██████████| 18/18 [00:00<00:00, 1124.88it/s]\n",
      "Epoch 539/700: 100%|██████████| 18/18 [00:00<00:00, 967.02it/s]\n",
      "Epoch 540/700: 100%|██████████| 18/18 [00:00<00:00, 833.61it/s]\n",
      "Epoch 541/700: 100%|██████████| 18/18 [00:00<00:00, 739.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [541/700], Training Loss: 0.1349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 542/700: 100%|██████████| 18/18 [00:00<00:00, 747.90it/s]\n",
      "Epoch 543/700: 100%|██████████| 18/18 [00:00<00:00, 749.38it/s]\n",
      "Epoch 544/700: 100%|██████████| 18/18 [00:00<00:00, 992.08it/s]\n",
      "Epoch 545/700: 100%|██████████| 18/18 [00:00<00:00, 764.36it/s]\n",
      "Epoch 546/700: 100%|██████████| 18/18 [00:00<00:00, 1124.61it/s]\n",
      "Epoch 547/700: 100%|██████████| 18/18 [00:00<00:00, 1132.75it/s]\n",
      "Epoch 548/700: 100%|██████████| 18/18 [00:00<00:00, 1118.93it/s]\n",
      "Epoch 549/700: 100%|██████████| 18/18 [00:00<00:00, 749.12it/s]\n",
      "Epoch 550/700: 100%|██████████| 18/18 [00:00<00:00, 750.36it/s]\n",
      "Epoch 551/700: 100%|██████████| 18/18 [00:00<00:00, 748.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [551/700], Training Loss: 0.1328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 552/700: 100%|██████████| 18/18 [00:00<00:00, 744.03it/s]\n",
      "Epoch 553/700: 100%|██████████| 18/18 [00:00<00:00, 748.94it/s]\n",
      "Epoch 554/700: 100%|██████████| 18/18 [00:00<00:00, 730.67it/s]\n",
      "Epoch 555/700: 100%|██████████| 18/18 [00:00<00:00, 1155.17it/s]\n",
      "Epoch 556/700: 100%|██████████| 18/18 [00:00<00:00, 1052.21it/s]\n",
      "Epoch 557/700: 100%|██████████| 18/18 [00:00<00:00, 1094.39it/s]\n",
      "Epoch 558/700: 100%|██████████| 18/18 [00:00<00:00, 1124.61it/s]\n",
      "Epoch 559/700: 100%|██████████| 18/18 [00:00<00:00, 734.28it/s]\n",
      "Epoch 560/700: 100%|██████████| 18/18 [00:00<00:00, 745.55it/s]\n",
      "Epoch 561/700: 100%|██████████| 18/18 [00:00<00:00, 749.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [561/700], Training Loss: 0.1344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 562/700: 100%|██████████| 18/18 [00:00<00:00, 320.24it/s]\n",
      "Epoch 563/700: 100%|██████████| 18/18 [00:00<00:00, 559.29it/s]\n",
      "Epoch 564/700: 100%|██████████| 18/18 [00:00<00:00, 1122.84it/s]\n",
      "Epoch 565/700: 100%|██████████| 18/18 [00:00<00:00, 1157.60it/s]\n",
      "Epoch 566/700: 100%|██████████| 18/18 [00:00<00:00, 812.34it/s]\n",
      "Epoch 567/700: 100%|██████████| 18/18 [00:00<00:00, 742.62it/s]\n",
      "Epoch 568/700: 100%|██████████| 18/18 [00:00<00:00, 548.07it/s]\n",
      "Epoch 569/700: 100%|██████████| 18/18 [00:00<00:00, 816.35it/s]\n",
      "Epoch 570/700: 100%|██████████| 18/18 [00:00<00:00, 749.64it/s]\n",
      "Epoch 571/700: 100%|██████████| 18/18 [00:00<00:00, 562.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [571/700], Training Loss: 0.1373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 572/700: 100%|██████████| 18/18 [00:00<00:00, 730.27it/s]\n",
      "Epoch 573/700: 100%|██████████| 18/18 [00:00<00:00, 583.78it/s]\n",
      "Epoch 574/700: 100%|██████████| 18/18 [00:00<00:00, 723.23it/s]\n",
      "Epoch 575/700: 100%|██████████| 18/18 [00:00<00:00, 636.74it/s]\n",
      "Epoch 576/700: 100%|██████████| 18/18 [00:00<00:00, 747.26it/s]\n",
      "Epoch 577/700: 100%|██████████| 18/18 [00:00<00:00, 750.42it/s]\n",
      "Epoch 578/700: 100%|██████████| 18/18 [00:00<00:00, 1110.27it/s]\n",
      "Epoch 579/700: 100%|██████████| 18/18 [00:00<00:00, 1156.75it/s]\n",
      "Epoch 580/700: 100%|██████████| 18/18 [00:00<00:00, 1120.99it/s]\n",
      "Epoch 581/700: 100%|██████████| 18/18 [00:00<00:00, 748.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [581/700], Training Loss: 0.1378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 582/700: 100%|██████████| 18/18 [00:00<00:00, 1082.48it/s]\n",
      "Epoch 583/700: 100%|██████████| 18/18 [00:00<00:00, 740.11it/s]\n",
      "Epoch 584/700: 100%|██████████| 18/18 [00:00<00:00, 1125.42it/s]\n",
      "Epoch 585/700: 100%|██████████| 18/18 [00:00<00:00, 734.99it/s]\n",
      "Epoch 586/700: 100%|██████████| 18/18 [00:00<00:00, 921.07it/s]\n",
      "Epoch 587/700: 100%|██████████| 18/18 [00:00<00:00, 964.68it/s]\n",
      "Epoch 588/700: 100%|██████████| 18/18 [00:00<00:00, 1118.65it/s]\n",
      "Epoch 589/700: 100%|██████████| 18/18 [00:00<00:00, 1130.39it/s]\n",
      "Epoch 590/700: 100%|██████████| 18/18 [00:00<00:00, 560.59it/s]\n",
      "Epoch 591/700: 100%|██████████| 18/18 [00:00<00:00, 559.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [591/700], Training Loss: 0.1394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 592/700: 100%|██████████| 18/18 [00:00<00:00, 750.82it/s]\n",
      "Epoch 593/700: 100%|██████████| 18/18 [00:00<00:00, 1126.02it/s]\n",
      "Epoch 594/700: 100%|██████████| 18/18 [00:00<00:00, 737.16it/s]\n",
      "Epoch 595/700: 100%|██████████| 18/18 [00:00<00:00, 1102.22it/s]\n",
      "Epoch 596/700: 100%|██████████| 18/18 [00:00<00:00, 747.44it/s]\n",
      "Epoch 597/700: 100%|██████████| 18/18 [00:00<00:00, 1072.79it/s]\n",
      "Epoch 598/700: 100%|██████████| 18/18 [00:00<00:00, 764.23it/s]\n",
      "Epoch 599/700: 100%|██████████| 18/18 [00:00<00:00, 1123.09it/s]\n",
      "Epoch 600/700: 100%|██████████| 18/18 [00:00<00:00, 1233.12it/s]\n",
      "Epoch 601/700: 100%|██████████| 18/18 [00:00<00:00, 1103.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [601/700], Training Loss: 0.1413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 602/700: 100%|██████████| 18/18 [00:00<00:00, 737.34it/s]\n",
      "Epoch 603/700: 100%|██████████| 18/18 [00:00<00:00, 730.34it/s]\n",
      "Epoch 604/700: 100%|██████████| 18/18 [00:00<00:00, 724.47it/s]\n",
      "Epoch 605/700: 100%|██████████| 18/18 [00:00<00:00, 1123.21it/s]\n",
      "Epoch 606/700: 100%|██████████| 18/18 [00:00<00:00, 747.56it/s]\n",
      "Epoch 607/700: 100%|██████████| 18/18 [00:00<00:00, 738.17it/s]\n",
      "Epoch 608/700: 100%|██████████| 18/18 [00:00<00:00, 548.50it/s]\n",
      "Epoch 609/700: 100%|██████████| 18/18 [00:00<00:00, 1102.85it/s]\n",
      "Epoch 610/700: 100%|██████████| 18/18 [00:00<00:00, 1098.53it/s]\n",
      "Epoch 611/700: 100%|██████████| 18/18 [00:00<00:00, 1125.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [611/700], Training Loss: 0.1358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 612/700: 100%|██████████| 18/18 [00:00<00:00, 747.18it/s]\n",
      "Epoch 613/700: 100%|██████████| 18/18 [00:00<00:00, 1121.80it/s]\n",
      "Epoch 614/700: 100%|██████████| 18/18 [00:00<00:00, 746.28it/s]\n",
      "Epoch 615/700: 100%|██████████| 18/18 [00:00<00:00, 1120.59it/s]\n",
      "Epoch 616/700: 100%|██████████| 18/18 [00:00<00:00, 1115.44it/s]\n",
      "Epoch 617/700: 100%|██████████| 18/18 [00:00<00:00, 1197.02it/s]\n",
      "Epoch 618/700: 100%|██████████| 18/18 [00:00<00:00, 449.56it/s]\n",
      "Epoch 619/700: 100%|██████████| 18/18 [00:00<00:00, 561.53it/s]\n",
      "Epoch 620/700: 100%|██████████| 18/18 [00:00<00:00, 781.28it/s]\n",
      "Epoch 621/700: 100%|██████████| 18/18 [00:00<00:00, 1053.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [621/700], Training Loss: 0.1331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 622/700: 100%|██████████| 18/18 [00:00<00:00, 745.10it/s]\n",
      "Epoch 623/700: 100%|██████████| 18/18 [00:00<00:00, 720.86it/s]\n",
      "Epoch 624/700: 100%|██████████| 18/18 [00:00<00:00, 749.09it/s]\n",
      "Epoch 625/700: 100%|██████████| 18/18 [00:00<00:00, 750.24it/s]\n",
      "Epoch 626/700: 100%|██████████| 18/18 [00:00<00:00, 1124.80it/s]\n",
      "Epoch 627/700: 100%|██████████| 18/18 [00:00<00:00, 740.60it/s]\n",
      "Epoch 628/700: 100%|██████████| 18/18 [00:00<00:00, 888.69it/s]\n",
      "Epoch 629/700: 100%|██████████| 18/18 [00:00<00:00, 750.86it/s]\n",
      "Epoch 630/700: 100%|██████████| 18/18 [00:00<00:00, 665.50it/s]\n",
      "Epoch 631/700: 100%|██████████| 18/18 [00:00<00:00, 1118.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [631/700], Training Loss: 0.1431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 632/700: 100%|██████████| 18/18 [00:00<00:00, 748.92it/s]\n",
      "Epoch 633/700: 100%|██████████| 18/18 [00:00<00:00, 1092.88it/s]\n",
      "Epoch 634/700: 100%|██████████| 18/18 [00:00<00:00, 1119.56it/s]\n",
      "Epoch 635/700: 100%|██████████| 18/18 [00:00<00:00, 1118.80it/s]\n",
      "Epoch 636/700: 100%|██████████| 18/18 [00:00<00:00, 748.52it/s]\n",
      "Epoch 637/700: 100%|██████████| 18/18 [00:00<00:00, 751.56it/s]\n",
      "Epoch 638/700: 100%|██████████| 18/18 [00:00<00:00, 1103.67it/s]\n",
      "Epoch 639/700: 100%|██████████| 18/18 [00:00<00:00, 1071.54it/s]\n",
      "Epoch 640/700: 100%|██████████| 18/18 [00:00<00:00, 754.42it/s]\n",
      "Epoch 641/700: 100%|██████████| 18/18 [00:00<00:00, 747.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [641/700], Training Loss: 0.1358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 642/700: 100%|██████████| 18/18 [00:00<00:00, 749.30it/s]\n",
      "Epoch 643/700: 100%|██████████| 18/18 [00:00<00:00, 712.99it/s]\n",
      "Epoch 644/700: 100%|██████████| 18/18 [00:00<00:00, 1123.54it/s]\n",
      "Epoch 645/700: 100%|██████████| 18/18 [00:00<00:00, 734.35it/s]\n",
      "Epoch 646/700: 100%|██████████| 18/18 [00:00<00:00, 373.56it/s]\n",
      "Epoch 647/700: 100%|██████████| 18/18 [00:00<00:00, 1157.79it/s]\n",
      "Epoch 648/700: 100%|██████████| 18/18 [00:00<00:00, 813.03it/s]\n",
      "Epoch 649/700: 100%|██████████| 18/18 [00:00<00:00, 1124.80it/s]\n",
      "Epoch 650/700: 100%|██████████| 18/18 [00:00<00:00, 733.49it/s]\n",
      "Epoch 651/700: 100%|██████████| 18/18 [00:00<00:00, 748.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [651/700], Training Loss: 0.1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 652/700: 100%|██████████| 18/18 [00:00<00:00, 751.07it/s]\n",
      "Epoch 653/700: 100%|██████████| 18/18 [00:00<00:00, 750.03it/s]\n",
      "Epoch 654/700: 100%|██████████| 18/18 [00:00<00:00, 786.07it/s]\n",
      "Epoch 655/700: 100%|██████████| 18/18 [00:00<00:00, 1098.02it/s]\n",
      "Epoch 656/700: 100%|██████████| 18/18 [00:00<00:00, 736.14it/s]\n",
      "Epoch 657/700: 100%|██████████| 18/18 [00:00<00:00, 1123.46it/s]\n",
      "Epoch 658/700: 100%|██████████| 18/18 [00:00<00:00, 1126.09it/s]\n",
      "Epoch 659/700: 100%|██████████| 18/18 [00:00<00:00, 1153.51it/s]\n",
      "Epoch 660/700: 100%|██████████| 18/18 [00:00<00:00, 748.84it/s]\n",
      "Epoch 661/700: 100%|██████████| 18/18 [00:00<00:00, 749.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [661/700], Training Loss: 0.1370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 662/700: 100%|██████████| 18/18 [00:00<00:00, 735.33it/s]\n",
      "Epoch 663/700: 100%|██████████| 18/18 [00:00<00:00, 747.34it/s]\n",
      "Epoch 664/700: 100%|██████████| 18/18 [00:00<00:00, 733.98it/s]\n",
      "Epoch 665/700: 100%|██████████| 18/18 [00:00<00:00, 751.14it/s]\n",
      "Epoch 666/700: 100%|██████████| 18/18 [00:00<00:00, 1104.81it/s]\n",
      "Epoch 667/700: 100%|██████████| 18/18 [00:00<00:00, 734.84it/s]\n",
      "Epoch 668/700: 100%|██████████| 18/18 [00:00<00:00, 1119.29it/s]\n",
      "Epoch 669/700: 100%|██████████| 18/18 [00:00<00:00, 736.88it/s]\n",
      "Epoch 670/700: 100%|██████████| 18/18 [00:00<00:00, 815.22it/s]\n",
      "Epoch 671/700: 100%|██████████| 18/18 [00:00<00:00, 745.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [671/700], Training Loss: 0.1355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 672/700: 100%|██████████| 18/18 [00:00<00:00, 749.22it/s]\n",
      "Epoch 673/700: 100%|██████████| 18/18 [00:00<00:00, 1121.74it/s]\n",
      "Epoch 674/700: 100%|██████████| 18/18 [00:00<00:00, 379.05it/s]\n",
      "Epoch 675/700: 100%|██████████| 18/18 [00:00<00:00, 724.84it/s]\n",
      "Epoch 676/700: 100%|██████████| 18/18 [00:00<00:00, 1122.94it/s]\n",
      "Epoch 677/700: 100%|██████████| 18/18 [00:00<00:00, 1124.76it/s]\n",
      "Epoch 678/700: 100%|██████████| 18/18 [00:00<00:00, 745.53it/s]\n",
      "Epoch 679/700: 100%|██████████| 18/18 [00:00<00:00, 738.43it/s]\n",
      "Epoch 680/700: 100%|██████████| 18/18 [00:00<00:00, 750.46it/s]\n",
      "Epoch 681/700: 100%|██████████| 18/18 [00:00<00:00, 1123.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [681/700], Training Loss: 0.1338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 682/700: 100%|██████████| 18/18 [00:00<00:00, 1111.14it/s]\n",
      "Epoch 683/700: 100%|██████████| 18/18 [00:00<00:00, 749.45it/s]\n",
      "Epoch 684/700: 100%|██████████| 18/18 [00:00<00:00, 1127.37it/s]\n",
      "Epoch 685/700: 100%|██████████| 18/18 [00:00<00:00, 1121.04it/s]\n",
      "Epoch 686/700: 100%|██████████| 18/18 [00:00<00:00, 1119.56it/s]\n",
      "Epoch 687/700: 100%|██████████| 18/18 [00:00<00:00, 748.53it/s]\n",
      "Epoch 688/700: 100%|██████████| 18/18 [00:00<00:00, 460.47it/s]\n",
      "Epoch 689/700: 100%|██████████| 18/18 [00:00<00:00, 1069.87it/s]\n",
      "Epoch 690/700: 100%|██████████| 18/18 [00:00<00:00, 1113.71it/s]\n",
      "Epoch 691/700: 100%|██████████| 18/18 [00:00<00:00, 748.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [691/700], Training Loss: 0.1422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 692/700: 100%|██████████| 18/18 [00:00<00:00, 1073.11it/s]\n",
      "Epoch 693/700: 100%|██████████| 18/18 [00:00<00:00, 538.58it/s]\n",
      "Epoch 694/700: 100%|██████████| 18/18 [00:00<00:00, 780.33it/s]\n",
      "Epoch 695/700: 100%|██████████| 18/18 [00:00<00:00, 1119.73it/s]\n",
      "Epoch 696/700: 100%|██████████| 18/18 [00:00<00:00, 752.21it/s]\n",
      "Epoch 697/700: 100%|██████████| 18/18 [00:00<00:00, 745.88it/s]\n",
      "Epoch 698/700: 100%|██████████| 18/18 [00:00<00:00, 748.97it/s]\n",
      "Epoch 699/700: 100%|██████████| 18/18 [00:00<00:00, 747.30it/s]\n",
      "Epoch 700/700: 100%|██████████| 18/18 [00:00<00:00, 731.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1741, Accuracy: 46.85%\n",
      "Final Validation Loss with MSE: 0.1741\n",
      "Final Validation Accuracy with MSE: 46.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import MSELoss\n",
    "\n",
    "# TODO: Train the model\n",
    "\n",
    "# TODO: Evaluate the model\n",
    "\n",
    "model = SimpleMLP(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, last_layer_activation_fn=None)\n",
    "criterion = MSELoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "trainer = SimpleMLPTrainer(model, criterion, optimizer)\n",
    "\n",
    "training_losses = trainer.train(train_loader, num_epochs=700)\n",
    "\n",
    "val_loss, val_accuracy = trainer.evaluate(val_loader)\n",
    "\n",
    "print(f\"Final Validation Loss with MSE: {val_loss:.4f}\")\n",
    "print(f\"Final Validation Accuracy with MSE: {val_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. NLLLoss (`torch.nn.NLLLoss`)\n",
    "- **Description:** Negative Log-Likelihood Loss measures the likelihood of the target class under the predicted probability distribution.\n",
    "- **Use Case:** Typically used in multi-class classification tasks, especially when combined with `log_softmax` activation.\n",
    "\n",
    "Here is the mathematical formulation of NLLLoss:\n",
    "\\begin{equation}\n",
    "\\text{NLLLoss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\log(y_{i})\n",
    "\\end{equation}\n",
    "\n",
    "I hope you note the logarithm in the formula. It's important! \n",
    "\n",
    "Why?\n",
    "\n",
    "In this part, run your training with Relu at last layer. <span style=\"color:red; font-weight: bold;\">Discuss </span> and explain the difference between the results of the two models. Find a proper solution to the problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sigmoid activation for BCELoss\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_hidden_layers=1, last_layer_activation_fn=nn.ReLU):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "        if last_layer_activation_fn is not None:\n",
    "            layers.append(last_layer_activation_fn())\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "class SimpleMLPTrainer:\n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train(self, train_loader, num_epochs):\n",
    "        training_losses = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            self.model.train()\n",
    "            for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                \n",
    "                # Apply sigmoid activation for BCELoss\n",
    "                probs = torch.sigmoid(outputs)\n",
    "                \n",
    "                # Calculate BCELoss\n",
    "                loss = self.criterion(probs, targets)\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            epoch_loss /= len(train_loader.dataset)\n",
    "            training_losses.append(epoch_loss)\n",
    "            if (epoch % 10 == 0):\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        return training_losses\n",
    "\n",
    "    def evaluate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = self.model(inputs)\n",
    "                \n",
    "                # Apply sigmoid activation for BCELoss\n",
    "                probs = torch.sigmoid(outputs)\n",
    "                \n",
    "                loss = self.criterion(probs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                predictions = (probs >= 0.5).float()\n",
    "                correct_predictions += (predictions == targets).sum().item()\n",
    "                total_predictions += targets.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "\n",
    "        print(f\"Final Validation Loss with BCELoss: {val_loss:.4f}\")\n",
    "        print(f\"Final Validation Accuracy with BCELoss: {accuracy * 100:.2f}%\")\n",
    "        return val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 18/18 [00:00<00:00, 1128.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 0.9708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 18/18 [00:00<00:00, 1124.50it/s]\n",
      "Epoch 3/100: 100%|██████████| 18/18 [00:00<00:00, 1124.80it/s]\n",
      "Epoch 4/100: 100%|██████████| 18/18 [00:00<00:00, 2149.15it/s]\n",
      "Epoch 5/100: 100%|██████████| 18/18 [00:00<00:00, 2071.32it/s]\n",
      "Epoch 6/100: 100%|██████████| 18/18 [00:00<00:00, 1085.30it/s]\n",
      "Epoch 7/100: 100%|██████████| 18/18 [00:00<00:00, 734.05it/s]\n",
      "Epoch 8/100: 100%|██████████| 18/18 [00:00<00:00, 1098.53it/s]\n",
      "Epoch 9/100: 100%|██████████| 18/18 [00:00<00:00, 1097.94it/s]\n",
      "Epoch 10/100: 100%|██████████| 18/18 [00:00<00:00, 2930.35it/s]\n",
      "Epoch 11/100: 100%|██████████| 18/18 [00:00<00:00, 1070.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100], Training Loss: 0.9708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 18/18 [00:00<00:00, 1616.51it/s]\n",
      "Epoch 13/100: 100%|██████████| 18/18 [00:00<00:00, 722.07it/s]\n",
      "Epoch 14/100: 100%|██████████| 18/18 [00:00<00:00, 1092.54it/s]\n",
      "Epoch 15/100: 100%|██████████| 18/18 [00:00<00:00, 910.99it/s]\n",
      "Epoch 16/100: 100%|██████████| 18/18 [00:00<00:00, 1286.90it/s]\n",
      "Epoch 17/100: 100%|██████████| 18/18 [00:00<00:00, 2337.74it/s]\n",
      "Epoch 18/100: 100%|██████████| 18/18 [00:00<00:00, 2357.60it/s]\n",
      "Epoch 19/100: 100%|██████████| 18/18 [00:00<00:00, 1069.48it/s]\n",
      "Epoch 20/100: 100%|██████████| 18/18 [00:00<00:00, 1097.84it/s]\n",
      "Epoch 21/100: 100%|██████████| 18/18 [00:00<00:00, 1142.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100], Training Loss: 0.9708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|██████████| 18/18 [00:00<00:00, 2170.78it/s]\n",
      "Epoch 23/100: 100%|██████████| 18/18 [00:00<00:00, 2157.07it/s]\n",
      "Epoch 24/100: 100%|██████████| 18/18 [00:00<00:00, 2074.11it/s]\n",
      "Epoch 25/100: 100%|██████████| 18/18 [00:00<00:00, 1086.11it/s]\n",
      "Epoch 26/100: 100%|██████████| 18/18 [00:00<00:00, 1094.82it/s]\n",
      "Epoch 27/100: 100%|██████████| 18/18 [00:00<00:00, 1262.31it/s]\n",
      "Epoch 28/100: 100%|██████████| 18/18 [00:00<00:00, 1039.05it/s]\n",
      "Epoch 29/100: 100%|██████████| 18/18 [00:00<00:00, 965.63it/s]\n",
      "Epoch 30/100: 100%|██████████| 18/18 [00:00<00:00, 2257.02it/s]\n",
      "Epoch 31/100: 100%|██████████| 18/18 [00:00<00:00, 1999.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100], Training Loss: 0.9708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100: 100%|██████████| 18/18 [00:00<00:00, 1074.04it/s]\n",
      "Epoch 33/100: 100%|██████████| 18/18 [00:00<00:00, 1060.51it/s]\n",
      "Epoch 34/100: 100%|██████████| 18/18 [00:00<00:00, 1086.09it/s]\n",
      "Epoch 35/100: 100%|██████████| 18/18 [00:00<00:00, 742.74it/s]\n",
      "Epoch 36/100: 100%|██████████| 18/18 [00:00<00:00, 748.57it/s]\n",
      "Epoch 37/100: 100%|██████████| 18/18 [00:00<00:00, 1081.50it/s]\n",
      "Epoch 38/100: 100%|██████████| 18/18 [00:00<00:00, 529.51it/s]\n",
      "Epoch 39/100: 100%|██████████| 18/18 [00:00<00:00, 578.52it/s]\n",
      "Epoch 40/100: 100%|██████████| 18/18 [00:00<00:00, 553.00it/s]\n",
      "Epoch 41/100: 100%|██████████| 18/18 [00:00<00:00, 1104.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/100], Training Loss: 0.9708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/100: 100%|██████████| 18/18 [00:00<00:00, 2233.98it/s]\n",
      "Epoch 43/100: 100%|██████████| 18/18 [00:00<00:00, 1119.26it/s]\n",
      "Epoch 44/100: 100%|██████████| 18/18 [00:00<00:00, 1102.07it/s]\n",
      "Epoch 45/100: 100%|██████████| 18/18 [00:00<00:00, 1108.02it/s]\n",
      "Epoch 46/100: 100%|██████████| 18/18 [00:00<00:00, 1096.10it/s]\n",
      "Epoch 47/100: 100%|██████████| 18/18 [00:00<00:00, 1086.20it/s]\n",
      "Epoch 48/100: 100%|██████████| 18/18 [00:00<00:00, 1097.76it/s]\n",
      "Epoch 49/100: 100%|██████████| 18/18 [00:00<00:00, 970.50it/s]\n",
      "Epoch 50/100: 100%|██████████| 18/18 [00:00<00:00, 1109.15it/s]\n",
      "Epoch 51/100: 100%|██████████| 18/18 [00:00<00:00, 1125.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/100], Training Loss: 0.9708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/100: 100%|██████████| 18/18 [00:00<00:00, 721.11it/s]\n",
      "Epoch 53/100: 100%|██████████| 18/18 [00:00<00:00, 1092.58it/s]\n",
      "Epoch 54/100: 100%|██████████| 18/18 [00:00<00:00, 1112.53it/s]\n",
      "Epoch 55/100: 100%|██████████| 18/18 [00:00<00:00, 719.33it/s]\n",
      "Epoch 56/100: 100%|██████████| 18/18 [00:00<00:00, 1117.59it/s]\n",
      "Epoch 57/100: 100%|██████████| 18/18 [00:00<00:00, 1124.63it/s]\n",
      "Epoch 58/100: 100%|██████████| 18/18 [00:00<00:00, 738.63it/s]\n",
      "Epoch 59/100: 100%|██████████| 18/18 [00:00<00:00, 740.59it/s]\n",
      "Epoch 60/100: 100%|██████████| 18/18 [00:00<00:00, 749.09it/s]\n",
      "Epoch 61/100: 100%|██████████| 18/18 [00:00<00:00, 1104.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/100], Training Loss: 0.9708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/100: 100%|██████████| 18/18 [00:00<00:00, 1223.80it/s]\n",
      "Epoch 63/100: 100%|██████████| 18/18 [00:00<00:00, 1066.92it/s]\n",
      "Epoch 64/100: 100%|██████████| 18/18 [00:00<00:00, 751.72it/s]\n",
      "Epoch 65/100: 100%|██████████| 18/18 [00:00<00:00, 986.29it/s]\n",
      "Epoch 66/100: 100%|██████████| 18/18 [00:00<00:00, 747.92it/s]\n",
      "Epoch 67/100: 100%|██████████| 18/18 [00:00<00:00, 541.88it/s]\n",
      "Epoch 68/100: 100%|██████████| 18/18 [00:00<00:00, 1121.24it/s]\n",
      "Epoch 69/100: 100%|██████████| 18/18 [00:00<00:00, 1109.00it/s]\n",
      "Epoch 70/100: 100%|██████████| 18/18 [00:00<00:00, 1117.84it/s]\n",
      "Epoch 71/100: 100%|██████████| 18/18 [00:00<00:00, 1088.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/100], Training Loss: 0.9708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/100: 100%|██████████| 18/18 [00:00<00:00, 1098.29it/s]\n",
      "Epoch 73/100: 100%|██████████| 18/18 [00:00<00:00, 1123.76it/s]\n",
      "Epoch 74/100: 100%|██████████| 18/18 [00:00<00:00, 700.05it/s]\n",
      "Epoch 75/100: 100%|██████████| 18/18 [00:00<00:00, 815.29it/s]\n",
      "Epoch 76/100: 100%|██████████| 18/18 [00:00<00:00, 1117.34it/s]\n",
      "Epoch 77/100: 100%|██████████| 18/18 [00:00<00:00, 1124.36it/s]\n",
      "Epoch 78/100: 100%|██████████| 18/18 [00:00<00:00, 1123.39it/s]\n",
      "Epoch 79/100: 100%|██████████| 18/18 [00:00<00:00, 748.72it/s]\n",
      "Epoch 80/100: 100%|██████████| 18/18 [00:00<00:00, 901.93it/s]\n",
      "Epoch 81/100: 100%|██████████| 18/18 [00:00<00:00, 1176.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [81/100], Training Loss: 0.9708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/100: 100%|██████████| 18/18 [00:00<00:00, 724.72it/s]\n",
      "Epoch 83/100: 100%|██████████| 18/18 [00:00<00:00, 1115.90it/s]\n",
      "Epoch 84/100: 100%|██████████| 18/18 [00:00<00:00, 1031.30it/s]\n",
      "Epoch 85/100: 100%|██████████| 18/18 [00:00<00:00, 1126.34it/s]\n",
      "Epoch 86/100: 100%|██████████| 18/18 [00:00<00:00, 717.38it/s]\n",
      "Epoch 87/100: 100%|██████████| 18/18 [00:00<00:00, 1135.74it/s]\n",
      "Epoch 88/100: 100%|██████████| 18/18 [00:00<00:00, 2152.40it/s]\n",
      "Epoch 89/100: 100%|██████████| 18/18 [00:00<00:00, 1103.99it/s]\n",
      "Epoch 90/100: 100%|██████████| 18/18 [00:00<00:00, 1149.11it/s]\n",
      "Epoch 91/100: 100%|██████████| 18/18 [00:00<00:00, 745.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [91/100], Training Loss: 0.9708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/100: 100%|██████████| 18/18 [00:00<00:00, 1127.94it/s]\n",
      "Epoch 93/100: 100%|██████████| 18/18 [00:00<00:00, 1095.23it/s]\n",
      "Epoch 94/100: 100%|██████████| 18/18 [00:00<00:00, 749.38it/s]\n",
      "Epoch 95/100: 100%|██████████| 18/18 [00:00<00:00, 1422.66it/s]\n",
      "Epoch 96/100: 100%|██████████| 18/18 [00:00<00:00, 559.12it/s]\n",
      "Epoch 97/100: 100%|██████████| 18/18 [00:00<00:00, 722.64it/s]\n",
      "Epoch 98/100: 100%|██████████| 18/18 [00:00<00:00, 704.43it/s]\n",
      "Epoch 99/100: 100%|██████████| 18/18 [00:00<00:00, 1373.01it/s]\n",
      "Epoch 100/100: 100%|██████████| 18/18 [00:00<00:00, 1128.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Loss with BCELoss: 0.8931\n",
      "Final Validation Accuracy with BCELoss: 39.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run with relu activation function\n",
    "from torch.nn import ReLU, NLLLoss, CrossEntropyLoss\n",
    "# TODO: Train the model\n",
    "# TODO: Evaluate the model\n",
    "\n",
    "criterion = nn.BCELoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 16\n",
    "output_dim = 1  \n",
    "model = SimpleMLP(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "trainer = SimpleMLPTrainer(model, criterion, optimizer)\n",
    "\n",
    "training_losses = trainer.train(train_loader, num_epochs=100)\n",
    "val_loss, val_accuracy = trainer.evaluate(val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with LogSoftmax activation function\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_hidden_layers=1, last_layer_activation_fn=nn.LogSoftmax):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "        if last_layer_activation_fn is not None:\n",
    "            layers.append(last_layer_activation_fn(dim=1))  \n",
    "            \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "class SimpleMLPTrainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def manual_nll_loss(self, log_probs, targets):\n",
    "        # Manually compute NLLLoss\n",
    "        if targets.dim() > 1:\n",
    "            targets = targets.squeeze()\n",
    "        targets = targets.long()  # Convert to LongTensor if needed\n",
    "        batch_size = log_probs.size(0)\n",
    "        target_log_probs = log_probs[range(batch_size), targets]  # log prob for correct class\n",
    "        return -target_log_probs.mean()\n",
    "\n",
    "    def train(self, train_loader, num_epochs):\n",
    "        training_losses = []\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            self.model.train()\n",
    "            for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "                self.optimizer.zero_grad()\n",
    "                log_probs = self.model(inputs) \n",
    "                loss = self.manual_nll_loss(log_probs, targets)  \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item() * inputs.size(0)\n",
    "            epoch_loss /= len(train_loader.dataset)\n",
    "            training_losses.append(epoch_loss)\n",
    "            if (epoch % 10 == 0):\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}\")\n",
    "        return training_losses\n",
    "\n",
    "    def evaluate(self, val_loader):\n",
    "  \n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "            \n",
    "                targets = targets.squeeze().long()\n",
    "                \n",
    "                log_probs = self.model(inputs)\n",
    "                loss = self.manual_nll_loss(log_probs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                _, predicted = torch.max(log_probs, 1)\n",
    "                \n",
    "                correct_predictions += (predicted == targets).sum().item()\n",
    "                total_predictions += targets.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        accuracy = (correct_predictions / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "        return val_loss, accuracy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 18/18 [00:00<00:00, 562.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Training Loss: 1.6479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████| 18/18 [00:00<00:00, 748.85it/s]\n",
      "Epoch 3/30: 100%|██████████| 18/18 [00:00<00:00, 754.93it/s]\n",
      "Epoch 4/30: 100%|██████████| 18/18 [00:00<00:00, 917.48it/s]\n",
      "Epoch 5/30: 100%|██████████| 18/18 [00:00<00:00, 741.20it/s]\n",
      "Epoch 6/30: 100%|██████████| 18/18 [00:00<00:00, 1084.42it/s]\n",
      "Epoch 7/30: 100%|██████████| 18/18 [00:00<00:00, 719.18it/s]\n",
      "Epoch 8/30: 100%|██████████| 18/18 [00:00<00:00, 1117.39it/s]\n",
      "Epoch 9/30: 100%|██████████| 18/18 [00:00<00:00, 744.57it/s]\n",
      "Epoch 10/30: 100%|██████████| 18/18 [00:00<00:00, 453.96it/s]\n",
      "Epoch 11/30: 100%|██████████| 18/18 [00:00<00:00, 1115.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/30], Training Loss: 0.6107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|██████████| 18/18 [00:00<00:00, 753.14it/s]\n",
      "Epoch 13/30: 100%|██████████| 18/18 [00:00<00:00, 1125.20it/s]\n",
      "Epoch 14/30: 100%|██████████| 18/18 [00:00<00:00, 2151.91it/s]\n",
      "Epoch 15/30: 100%|██████████| 18/18 [00:00<00:00, 1105.82it/s]\n",
      "Epoch 16/30: 100%|██████████| 18/18 [00:00<00:00, 743.99it/s]\n",
      "Epoch 17/30: 100%|██████████| 18/18 [00:00<00:00, 733.34it/s]\n",
      "Epoch 18/30: 100%|██████████| 18/18 [00:00<00:00, 749.60it/s]\n",
      "Epoch 19/30: 100%|██████████| 18/18 [00:00<00:00, 1114.98it/s]\n",
      "Epoch 20/30: 100%|██████████| 18/18 [00:00<00:00, 761.69it/s]\n",
      "Epoch 21/30: 100%|██████████| 18/18 [00:00<00:00, 746.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/30], Training Loss: 0.5735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 100%|██████████| 18/18 [00:00<00:00, 549.70it/s]\n",
      "Epoch 23/30: 100%|██████████| 18/18 [00:00<00:00, 742.44it/s]\n",
      "Epoch 24/30: 100%|██████████| 18/18 [00:00<00:00, 749.95it/s]\n",
      "Epoch 25/30: 100%|██████████| 18/18 [00:00<00:00, 596.46it/s]\n",
      "Epoch 26/30: 100%|██████████| 18/18 [00:00<00:00, 556.14it/s]\n",
      "Epoch 27/30: 100%|██████████| 18/18 [00:00<00:00, 450.12it/s]\n",
      "Epoch 28/30: 100%|██████████| 18/18 [00:00<00:00, 544.75it/s]\n",
      "Epoch 29/30: 100%|██████████| 18/18 [00:00<00:00, 741.36it/s]\n",
      "Epoch 30/30: 100%|██████████| 18/18 [00:00<00:00, 552.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6203, Accuracy: 61.54%\n",
      "Final Validation Loss with manual NLLLoss: 0.6203\n",
      "Final Validation Accuracy with manual NLLLoss: 61.54%\n"
     ]
    }
   ],
   "source": [
    "# Run with LogSoftmax activation function\n",
    "from torch.nn import NLLLoss\n",
    "\n",
    "# TODO: Train the model\n",
    "\n",
    "# TODO: Evaluate the model\n",
    "model = SimpleMLP(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=2)  # 2 output nodes for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and evaluation\n",
    "trainer = SimpleMLPTrainer(model, optimizer)\n",
    "\n",
    "training_losses = trainer.train(train_loader, num_epochs=30)\n",
    "val_loss, val_accuracy = trainer.evaluate(val_loader)\n",
    "\n",
    "print(f\"Final Validation Loss with manual NLLLoss: {val_loss:.4f}\")\n",
    "print(f\"Final Validation Accuracy with manual NLLLoss: {val_accuracy :.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your reason for your choice:\n",
    "\n",
    "<div>\n",
    "**Your answer here**\n",
    "</div>\n",
    "\n",
    "When using ReLU as the last layer activation function, the model output values are non-negative but unbounded, meaning they don't naturally fall within a probability distribution range (i.e., [0,1]). This output isn't optimal for binary or multi-class classification tasks where we want outputs representing class probabilities.\n",
    "\n",
    "Impact of Using ReLU in the Last Layer: ReLU allows positive outputs but may produce values much greater than 1. When combined with loss functions like NLLLoss, this can lead to poor performance, as NLLLoss expects log-probabilities (typically from a log_softmax activation) rather than raw outputs.\n",
    "\n",
    "\n",
    "For binary or multi-class classification, a more suitable solution is to use sigmoid (for binary) or softmax (for multi-class) in the last layer. These functions output values between 0 and 1, representing probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. CrossEntropyLoss (`torch.nn.CrossEntropyLoss`)\n",
    "- **Description:** Combines `LogSoftmax` and `NLLLoss` in one single class. It computes the cross-entropy loss between the target and the output logits.\n",
    "- **Use Case:** Widely used for multi-class classification problems.\n",
    "\n",
    "The mathematical formulation of CrossEntropyLoss is as follows:\n",
    "\\begin{equation}\n",
    "  \\text{CrossEntropy}(y, \\hat{y}) = - \\sum_{i=1}^{C} y_i \\log\\left(\\frac{e^{\\hat{y}_i}}{\\sum_{j=1}^{C} e^{\\hat{y}_j}}\\right)\n",
    "\\end{equation}\n",
    "  where:\n",
    "  - \\( C \\) is the number of classes,\n",
    "  - \\( y_i \\) is a one-hot encoded target vector (or a scalar class label),\n",
    "  - \\( \\hat{y}_i \\) represents the logits (unnormalized model outputs) for each class.\n",
    "  \n",
    "  In practice, `torch.nn.CrossEntropyLoss` expects raw logits as input and internally applies the softmax function to convert the logits into probabilities, followed by the negative log-likelihood computation.\n",
    "\n",
    "- **Background:** Cross-entropy measures the difference between the true distribution \\( y \\) and the predicted distribution \\( \\hat{y} \\). The function minimizes the negative log-probability assigned to the correct class, effectively penalizing predictions that deviate from the true class, making it a standard choice for classification tasks in deep learning.\n",
    "\n",
    "Now, let's implement a class called `SimpleMLP_Loss` that has the following architecture:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP_Loss(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_hidden_layers=1):\n",
    "        super(SimpleMLP_Loss, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLPTrainer_Loss:\n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train(self, train_loader, num_epochs):\n",
    "        training_losses = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            self.model.train()\n",
    "            for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)  \n",
    "                \n",
    "                targets = targets.squeeze().long()\n",
    "                \n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            epoch_loss /= len(train_loader.dataset)\n",
    "            training_losses.append(epoch_loss)\n",
    "            if (epoch % 10 == 0):\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        return training_losses\n",
    "\n",
    "    def evaluate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = self.model(inputs)  \n",
    "                \n",
    "                targets = targets.squeeze().long()\n",
    "                \n",
    "                loss = self.criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "                correct_predictions += (predictions == targets).sum().item()\n",
    "                total_predictions += targets.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        accuracy = correct_predictions / total_predictions * 100 if total_predictions > 0 else 0\n",
    "\n",
    "        print(f\"Final Validation Loss with CrossEntropyLoss: {val_loss:.4f}\")\n",
    "        print(f\"Final Validation Accuracy with CrossEntropyLoss: {accuracy:.2f}%\")\n",
    "        return val_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 18/18 [00:00<00:00, 455.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 2.1591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 18/18 [00:00<00:00, 734.80it/s]\n",
      "Epoch 3/100: 100%|██████████| 18/18 [00:00<00:00, 554.98it/s]\n",
      "Epoch 4/100: 100%|██████████| 18/18 [00:00<00:00, 438.84it/s]\n",
      "Epoch 5/100: 100%|██████████| 18/18 [00:00<00:00, 726.91it/s]\n",
      "Epoch 6/100: 100%|██████████| 18/18 [00:00<00:00, 561.90it/s]\n",
      "Epoch 7/100: 100%|██████████| 18/18 [00:00<00:00, 438.29it/s]\n",
      "Epoch 8/100: 100%|██████████| 18/18 [00:00<00:00, 718.18it/s]\n",
      "Epoch 9/100: 100%|██████████| 18/18 [00:00<00:00, 554.23it/s]\n",
      "Epoch 10/100: 100%|██████████| 18/18 [00:00<00:00, 539.85it/s]\n",
      "Epoch 11/100: 100%|██████████| 18/18 [00:00<00:00, 747.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100], Training Loss: 0.5732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 18/18 [00:00<00:00, 743.44it/s]\n",
      "Epoch 13/100: 100%|██████████| 18/18 [00:00<00:00, 444.50it/s]\n",
      "Epoch 14/100: 100%|██████████| 18/18 [00:00<00:00, 564.51it/s]\n",
      "Epoch 15/100: 100%|██████████| 18/18 [00:00<00:00, 563.53it/s]\n",
      "Epoch 16/100: 100%|██████████| 18/18 [00:00<00:00, 394.44it/s]\n",
      "Epoch 17/100: 100%|██████████| 18/18 [00:00<00:00, 562.35it/s]\n",
      "Epoch 18/100: 100%|██████████| 18/18 [00:00<00:00, 588.55it/s]\n",
      "Epoch 19/100: 100%|██████████| 18/18 [00:00<00:00, 568.03it/s]\n",
      "Epoch 20/100: 100%|██████████| 18/18 [00:00<00:00, 560.15it/s]\n",
      "Epoch 21/100: 100%|██████████| 18/18 [00:00<00:00, 439.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100], Training Loss: 0.5561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|██████████| 18/18 [00:00<00:00, 445.89it/s]\n",
      "Epoch 23/100: 100%|██████████| 18/18 [00:00<00:00, 561.17it/s]\n",
      "Epoch 24/100: 100%|██████████| 18/18 [00:00<00:00, 562.11it/s]\n",
      "Epoch 25/100: 100%|██████████| 18/18 [00:00<00:00, 561.23it/s]\n",
      "Epoch 26/100: 100%|██████████| 18/18 [00:00<00:00, 607.54it/s]\n",
      "Epoch 27/100: 100%|██████████| 18/18 [00:00<00:00, 558.03it/s]\n",
      "Epoch 28/100: 100%|██████████| 18/18 [00:00<00:00, 734.43it/s]\n",
      "Epoch 29/100: 100%|██████████| 18/18 [00:00<00:00, 373.68it/s]\n",
      "Epoch 30/100: 100%|██████████| 18/18 [00:00<00:00, 457.05it/s]\n",
      "Epoch 31/100: 100%|██████████| 18/18 [00:00<00:00, 744.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100], Training Loss: 0.5491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100: 100%|██████████| 18/18 [00:00<00:00, 748.69it/s]\n",
      "Epoch 33/100: 100%|██████████| 18/18 [00:00<00:00, 562.23it/s]\n",
      "Epoch 34/100: 100%|██████████| 18/18 [00:00<00:00, 554.96it/s]\n",
      "Epoch 35/100: 100%|██████████| 18/18 [00:00<00:00, 555.96it/s]\n",
      "Epoch 36/100: 100%|██████████| 18/18 [00:00<00:00, 447.12it/s]\n",
      "Epoch 37/100: 100%|██████████| 18/18 [00:00<00:00, 563.19it/s]\n",
      "Epoch 38/100: 100%|██████████| 18/18 [00:00<00:00, 552.36it/s]\n",
      "Epoch 39/100: 100%|██████████| 18/18 [00:00<00:00, 562.28it/s]\n",
      "Epoch 40/100: 100%|██████████| 18/18 [00:00<00:00, 740.92it/s]\n",
      "Epoch 41/100: 100%|██████████| 18/18 [00:00<00:00, 753.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/100], Training Loss: 0.5244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/100: 100%|██████████| 18/18 [00:00<00:00, 371.51it/s]\n",
      "Epoch 43/100: 100%|██████████| 18/18 [00:00<00:00, 449.03it/s]\n",
      "Epoch 44/100: 100%|██████████| 18/18 [00:00<00:00, 402.15it/s]\n",
      "Epoch 45/100: 100%|██████████| 18/18 [00:00<00:00, 368.81it/s]\n",
      "Epoch 46/100: 100%|██████████| 18/18 [00:00<00:00, 373.13it/s]\n",
      "Epoch 47/100: 100%|██████████| 18/18 [00:00<00:00, 370.19it/s]\n",
      "Epoch 48/100: 100%|██████████| 18/18 [00:00<00:00, 538.74it/s]\n",
      "Epoch 49/100: 100%|██████████| 18/18 [00:00<00:00, 549.98it/s]\n",
      "Epoch 50/100: 100%|██████████| 18/18 [00:00<00:00, 563.60it/s]\n",
      "Epoch 51/100: 100%|██████████| 18/18 [00:00<00:00, 713.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/100], Training Loss: 0.5124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/100: 100%|██████████| 18/18 [00:00<00:00, 559.17it/s]\n",
      "Epoch 53/100: 100%|██████████| 18/18 [00:00<00:00, 554.83it/s]\n",
      "Epoch 54/100: 100%|██████████| 18/18 [00:00<00:00, 739.53it/s]\n",
      "Epoch 55/100: 100%|██████████| 18/18 [00:00<00:00, 369.38it/s]\n",
      "Epoch 56/100: 100%|██████████| 18/18 [00:00<00:00, 372.34it/s]\n",
      "Epoch 57/100: 100%|██████████| 18/18 [00:00<00:00, 558.24it/s]\n",
      "Epoch 58/100: 100%|██████████| 18/18 [00:00<00:00, 450.13it/s]\n",
      "Epoch 59/100: 100%|██████████| 18/18 [00:00<00:00, 397.00it/s]\n",
      "Epoch 60/100: 100%|██████████| 18/18 [00:00<00:00, 450.70it/s]\n",
      "Epoch 61/100: 100%|██████████| 18/18 [00:00<00:00, 542.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/100], Training Loss: 0.5080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/100: 100%|██████████| 18/18 [00:00<00:00, 555.33it/s]\n",
      "Epoch 63/100: 100%|██████████| 18/18 [00:00<00:00, 559.94it/s]\n",
      "Epoch 64/100: 100%|██████████| 18/18 [00:00<00:00, 561.57it/s]\n",
      "Epoch 65/100: 100%|██████████| 18/18 [00:00<00:00, 741.29it/s]\n",
      "Epoch 66/100: 100%|██████████| 18/18 [00:00<00:00, 551.81it/s]\n",
      "Epoch 67/100: 100%|██████████| 18/18 [00:00<00:00, 562.86it/s]\n",
      "Epoch 68/100: 100%|██████████| 18/18 [00:00<00:00, 747.44it/s]\n",
      "Epoch 69/100: 100%|██████████| 18/18 [00:00<00:00, 562.14it/s]\n",
      "Epoch 70/100: 100%|██████████| 18/18 [00:00<00:00, 749.44it/s]\n",
      "Epoch 71/100: 100%|██████████| 18/18 [00:00<00:00, 586.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/100], Training Loss: 0.4954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/100: 100%|██████████| 18/18 [00:00<00:00, 444.38it/s]\n",
      "Epoch 73/100: 100%|██████████| 18/18 [00:00<00:00, 436.96it/s]\n",
      "Epoch 74/100: 100%|██████████| 18/18 [00:00<00:00, 374.35it/s]\n",
      "Epoch 75/100: 100%|██████████| 18/18 [00:00<00:00, 502.46it/s]\n",
      "Epoch 76/100: 100%|██████████| 18/18 [00:00<00:00, 561.29it/s]\n",
      "Epoch 77/100: 100%|██████████| 18/18 [00:00<00:00, 435.08it/s]\n",
      "Epoch 78/100: 100%|██████████| 18/18 [00:00<00:00, 400.93it/s]\n",
      "Epoch 79/100: 100%|██████████| 18/18 [00:00<00:00, 373.78it/s]\n",
      "Epoch 80/100: 100%|██████████| 18/18 [00:00<00:00, 447.92it/s]\n",
      "Epoch 81/100: 100%|██████████| 18/18 [00:00<00:00, 444.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [81/100], Training Loss: 0.4748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/100: 100%|██████████| 18/18 [00:00<00:00, 369.44it/s]\n",
      "Epoch 83/100: 100%|██████████| 18/18 [00:00<00:00, 450.02it/s]\n",
      "Epoch 84/100: 100%|██████████| 18/18 [00:00<00:00, 548.86it/s]\n",
      "Epoch 85/100: 100%|██████████| 18/18 [00:00<00:00, 444.30it/s]\n",
      "Epoch 86/100: 100%|██████████| 18/18 [00:00<00:00, 444.19it/s]\n",
      "Epoch 87/100: 100%|██████████| 18/18 [00:00<00:00, 444.07it/s]\n",
      "Epoch 88/100: 100%|██████████| 18/18 [00:00<00:00, 494.28it/s]\n",
      "Epoch 89/100: 100%|██████████| 18/18 [00:00<00:00, 561.31it/s]\n",
      "Epoch 90/100: 100%|██████████| 18/18 [00:00<00:00, 733.69it/s]\n",
      "Epoch 91/100: 100%|██████████| 18/18 [00:00<00:00, 448.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [91/100], Training Loss: 0.4798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/100: 100%|██████████| 18/18 [00:00<00:00, 558.82it/s]\n",
      "Epoch 93/100: 100%|██████████| 18/18 [00:00<00:00, 552.84it/s]\n",
      "Epoch 94/100: 100%|██████████| 18/18 [00:00<00:00, 449.54it/s]\n",
      "Epoch 95/100: 100%|██████████| 18/18 [00:00<00:00, 557.38it/s]\n",
      "Epoch 96/100: 100%|██████████| 18/18 [00:00<00:00, 748.63it/s]\n",
      "Epoch 97/100: 100%|██████████| 18/18 [00:00<00:00, 543.48it/s]\n",
      "Epoch 98/100: 100%|██████████| 18/18 [00:00<00:00, 449.90it/s]\n",
      "Epoch 99/100: 100%|██████████| 18/18 [00:00<00:00, 559.90it/s]\n",
      "Epoch 100/100: 100%|██████████| 18/18 [00:00<00:00, 551.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Loss with CrossEntropyLoss: 0.5468\n",
      "Final Validation Accuracy with CrossEntropyLoss: 73.43%\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "#TODO Train the model\n",
    "\n",
    "#TODO Evaluate the model\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim =16\n",
    "output_dim = 2   \n",
    "\n",
    "model = SimpleMLP_Loss(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "trainer = SimpleMLPTrainer_Loss(model, criterion, optimizer)\n",
    "\n",
    "training_losses = trainer.train(train_loader, num_epochs=100)\n",
    "val_loss, val_accuracy = trainer.evaluate(val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. KLDivLoss (`torch.nn.KLDivLoss`)\n",
    "- **Description:** Kullback-Leibler Divergence Loss measures how one probability distribution diverges from a second, reference distribution. Unlike other loss functions that focus on classification, KL divergence specifically compares the relative entropy between two distributions. It quantifies the information loss when using the predicted distribution to approximate the true distribution. \n",
    "\n",
    "- **Mathematical Function:**\n",
    "\\begin{equation}\n",
    "  \\text{KL}(P \\parallel Q) = \\sum_{i=1}^{C} P(i) \\left( \\log P(i) - \\log Q(i) \\right)\n",
    "\\end{equation}\n",
    "  where:\n",
    "  - \\( P \\) is the target (true) probability distribution,\n",
    "  - \\( Q \\) is the predicted distribution (often the output of `log_softmax`),\n",
    "  - \\( C \\) is the number of classes.\n",
    "\n",
    "  KL divergence is always non-negative, and it equals zero if the two distributions are identical. The loss function expects the model's output to be in the form of log-probabilities (using `log_softmax`) and compares this against a target probability distribution, which is typically a normalized distribution (using softmax).\n",
    "\n",
    "- **Use Case:** KLDivLoss is frequently used in:\n",
    "  - **Variational Autoencoders (VAEs):** In VAEs, KL divergence is used to measure how much the learned latent space distribution deviates from a prior distribution (often Gaussian).\n",
    "  - **Knowledge Distillation:** In teacher-student models, KL divergence is used to transfer the \"soft\" knowledge from a teacher model to a student model by comparing their output probability distributions.\n",
    "  - **Reinforcement Learning:** It can be used to update policies while minimizing the divergence from a previous policy.\n",
    "\n",
    "- **Background:** Kullback-Leibler divergence, a core concept in information theory, measures the inefficiency of assuming the predicted distribution \\( Q \\) when the true distribution is \\( P \\). It is asymmetric, meaning that \\( KL(P \\parallel Q) \\neq KL(Q \\parallel P) \\), so the direction of the comparison matters.\n",
    "\n",
    "Again, in this part, run your training with Relu at last layer. <span style=\"color:red; font-weight: bold;\">Discuss </span> and explain the difference between the results of the two models. Find a proper solution to the problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP_KL(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_hidden_layers=1):\n",
    "        super(SimpleMLP_KL, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        layers.append(nn.LogSoftmax(dim=1))  \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class SimpleMLPTrainer_KL:\n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train(self, train_loader, num_epochs):\n",
    "        training_losses = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            self.model.train()\n",
    "            for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)  \n",
    "                target_probs = torch.softmax(targets, dim=1)\n",
    "                \n",
    "                loss = self.criterion(outputs, target_probs)\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            epoch_loss /= len(train_loader.dataset)\n",
    "            training_losses.append(epoch_loss)\n",
    "            if (epoch % 10 == 0):\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        return training_losses\n",
    "\n",
    "    def evaluate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = self.model(inputs)  # Log probabilities\n",
    "\n",
    "                # Convert targets to one-hot encoding and then to a probability distribution\n",
    "                targets = F.one_hot(targets.squeeze().long(), num_classes=outputs.size(1)).float()\n",
    "                \n",
    "                # Calculate KLDivLoss\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Convert log-probabilities to predicted class\n",
    "                predictions = torch.argmax(outputs.exp(), dim=1)  # Convert to probabilities and get max\n",
    "                correct_predictions += (predictions == targets.argmax(dim=1)).sum().item()\n",
    "                total_predictions += targets.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        accuracy = (correct_predictions / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "\n",
    "        print(f\"Final Validation Loss with KLDivLoss: {val_loss:.4f}\")\n",
    "        print(f\"Final Validation Accuracy with KLDivLoss: {accuracy:.2f}%\")\n",
    "        return val_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/80: 100%|██████████| 18/18 [00:00<00:00, 478.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Training Loss: 4.0629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/80: 100%|██████████| 18/18 [00:00<00:00, 544.24it/s]\n",
      "Epoch 3/80: 100%|██████████| 18/18 [00:00<00:00, 562.66it/s]\n",
      "Epoch 4/80: 100%|██████████| 18/18 [00:00<00:00, 735.74it/s]\n",
      "Epoch 5/80: 100%|██████████| 18/18 [00:00<00:00, 1106.06it/s]\n",
      "Epoch 6/80: 100%|██████████| 18/18 [00:00<00:00, 1124.78it/s]\n",
      "Epoch 7/80: 100%|██████████| 18/18 [00:00<00:00, 1125.94it/s]\n",
      "Epoch 8/80: 100%|██████████| 18/18 [00:00<00:00, 1122.81it/s]\n",
      "Epoch 9/80: 100%|██████████| 18/18 [00:00<00:00, 1125.03it/s]\n",
      "Epoch 10/80: 100%|██████████| 18/18 [00:00<00:00, 760.99it/s]\n",
      "Epoch 11/80: 100%|██████████| 18/18 [00:00<00:00, 1123.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/80], Training Loss: 1.4018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/80: 100%|██████████| 18/18 [00:00<00:00, 1123.62it/s]\n",
      "Epoch 13/80: 100%|██████████| 18/18 [00:00<00:00, 744.64it/s]\n",
      "Epoch 14/80: 100%|██████████| 18/18 [00:00<00:00, 940.18it/s]\n",
      "Epoch 15/80: 100%|██████████| 18/18 [00:00<00:00, 1375.38it/s]\n",
      "Epoch 16/80: 100%|██████████| 18/18 [00:00<00:00, 1124.68it/s]\n",
      "Epoch 17/80: 100%|██████████| 18/18 [00:00<00:00, 1088.27it/s]\n",
      "Epoch 18/80: 100%|██████████| 18/18 [00:00<00:00, 737.73it/s]\n",
      "Epoch 19/80: 100%|██████████| 18/18 [00:00<00:00, 1077.10it/s]\n",
      "Epoch 20/80: 100%|██████████| 18/18 [00:00<00:00, 1150.91it/s]\n",
      "Epoch 21/80: 100%|██████████| 18/18 [00:00<00:00, 1096.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/80], Training Loss: 1.3946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/80: 100%|██████████| 18/18 [00:00<00:00, 1140.86it/s]\n",
      "Epoch 23/80: 100%|██████████| 18/18 [00:00<00:00, 747.37it/s]\n",
      "Epoch 24/80: 100%|██████████| 18/18 [00:00<00:00, 841.70it/s]\n",
      "Epoch 25/80: 100%|██████████| 18/18 [00:00<00:00, 1155.60it/s]\n",
      "Epoch 26/80: 100%|██████████| 18/18 [00:00<00:00, 1089.37it/s]\n",
      "Epoch 27/80: 100%|██████████| 18/18 [00:00<00:00, 749.99it/s]\n",
      "Epoch 28/80: 100%|██████████| 18/18 [00:00<00:00, 1099.95it/s]\n",
      "Epoch 29/80: 100%|██████████| 18/18 [00:00<00:00, 1123.12it/s]\n",
      "Epoch 30/80: 100%|██████████| 18/18 [00:00<00:00, 1070.57it/s]\n",
      "Epoch 31/80: 100%|██████████| 18/18 [00:00<00:00, 548.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/80], Training Loss: 1.3907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/80: 100%|██████████| 18/18 [00:00<00:00, 721.13it/s]\n",
      "Epoch 33/80: 100%|██████████| 18/18 [00:00<00:00, 1135.56it/s]\n",
      "Epoch 34/80: 100%|██████████| 18/18 [00:00<00:00, 1092.30it/s]\n",
      "Epoch 35/80: 100%|██████████| 18/18 [00:00<00:00, 1106.51it/s]\n",
      "Epoch 36/80: 100%|██████████| 18/18 [00:00<00:00, 1089.67it/s]\n",
      "Epoch 37/80: 100%|██████████| 18/18 [00:00<00:00, 1159.61it/s]\n",
      "Epoch 38/80: 100%|██████████| 18/18 [00:00<00:00, 1103.91it/s]\n",
      "Epoch 39/80: 100%|██████████| 18/18 [00:00<00:00, 1090.64it/s]\n",
      "Epoch 40/80: 100%|██████████| 18/18 [00:00<00:00, 741.88it/s]\n",
      "Epoch 41/80: 100%|██████████| 18/18 [00:00<00:00, 1107.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/80], Training Loss: 1.3889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/80: 100%|██████████| 18/18 [00:00<00:00, 560.73it/s]\n",
      "Epoch 43/80: 100%|██████████| 18/18 [00:00<00:00, 554.92it/s]\n",
      "Epoch 44/80: 100%|██████████| 18/18 [00:00<00:00, 750.11it/s]\n",
      "Epoch 45/80: 100%|██████████| 18/18 [00:00<00:00, 750.12it/s]\n",
      "Epoch 46/80: 100%|██████████| 18/18 [00:00<00:00, 1119.69it/s]\n",
      "Epoch 47/80: 100%|██████████| 18/18 [00:00<00:00, 1065.33it/s]\n",
      "Epoch 48/80: 100%|██████████| 18/18 [00:00<00:00, 748.37it/s]\n",
      "Epoch 49/80: 100%|██████████| 18/18 [00:00<00:00, 1134.17it/s]\n",
      "Epoch 50/80: 100%|██████████| 18/18 [00:00<00:00, 1037.97it/s]\n",
      "Epoch 51/80: 100%|██████████| 18/18 [00:00<00:00, 757.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/80], Training Loss: 1.3884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/80: 100%|██████████| 18/18 [00:00<00:00, 1092.27it/s]\n",
      "Epoch 53/80: 100%|██████████| 18/18 [00:00<00:00, 721.44it/s]\n",
      "Epoch 54/80: 100%|██████████| 18/18 [00:00<00:00, 1097.92it/s]\n",
      "Epoch 55/80: 100%|██████████| 18/18 [00:00<00:00, 1107.28it/s]\n",
      "Epoch 56/80: 100%|██████████| 18/18 [00:00<00:00, 1107.16it/s]\n",
      "Epoch 57/80: 100%|██████████| 18/18 [00:00<00:00, 1157.69it/s]\n",
      "Epoch 58/80: 100%|██████████| 18/18 [00:00<00:00, 367.56it/s]\n",
      "Epoch 59/80: 100%|██████████| 18/18 [00:00<00:00, 756.66it/s]\n",
      "Epoch 60/80: 100%|██████████| 18/18 [00:00<00:00, 1081.33it/s]\n",
      "Epoch 61/80: 100%|██████████| 18/18 [00:00<00:00, 1089.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/80], Training Loss: 1.3874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/80: 100%|██████████| 18/18 [00:00<00:00, 1122.14it/s]\n",
      "Epoch 63/80: 100%|██████████| 18/18 [00:00<00:00, 1094.15it/s]\n",
      "Epoch 64/80: 100%|██████████| 18/18 [00:00<00:00, 1122.86it/s]\n",
      "Epoch 65/80: 100%|██████████| 18/18 [00:00<00:00, 1125.15it/s]\n",
      "Epoch 66/80: 100%|██████████| 18/18 [00:00<00:00, 1125.05it/s]\n",
      "Epoch 67/80: 100%|██████████| 18/18 [00:00<00:00, 1122.92it/s]\n",
      "Epoch 68/80: 100%|██████████| 18/18 [00:00<00:00, 922.07it/s]\n",
      "Epoch 69/80: 100%|██████████| 18/18 [00:00<00:00, 874.54it/s]\n",
      "Epoch 70/80: 100%|██████████| 18/18 [00:00<00:00, 670.96it/s]\n",
      "Epoch 71/80: 100%|██████████| 18/18 [00:00<00:00, 1124.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/80], Training Loss: 1.3868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/80: 100%|██████████| 18/18 [00:00<00:00, 747.62it/s]\n",
      "Epoch 73/80: 100%|██████████| 18/18 [00:00<00:00, 1083.24it/s]\n",
      "Epoch 74/80: 100%|██████████| 18/18 [00:00<00:00, 1156.11it/s]\n",
      "Epoch 75/80: 100%|██████████| 18/18 [00:00<00:00, 1157.81it/s]\n",
      "Epoch 76/80: 100%|██████████| 18/18 [00:00<00:00, 750.05it/s]\n",
      "Epoch 77/80: 100%|██████████| 18/18 [00:00<00:00, 734.10it/s]\n",
      "Epoch 78/80: 100%|██████████| 18/18 [00:00<00:00, 1092.90it/s]\n",
      "Epoch 79/80: 100%|██████████| 18/18 [00:00<00:00, 1096.04it/s]\n",
      "Epoch 80/80: 100%|██████████| 18/18 [00:00<00:00, 1122.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Loss with KLDivLoss: 0.6939\n",
      "Final Validation Accuracy with KLDivLoss: 53.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run with LogSoftmax activation function\n",
    "from torch.nn import NLLLoss\n",
    "\n",
    "# TODO: Train the model\n",
    "\n",
    "# TODO: Evaluate the model\n",
    "\n",
    "model = SimpleMLP_KL(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "criterion = nn.KLDivLoss(reduction='batchmean')  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "trainer = SimpleMLPTrainer_KL(model, criterion, optimizer)\n",
    "training_losses = trainer.train(train_loader, num_epochs=80)\n",
    "val_loss, val_accuracy = trainer.evaluate(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your reason for your choice:\n",
    "\n",
    "<div>\n",
    "**Your answer here**\n",
    "</div>\n",
    "\n",
    "To use KLDivLoss effectively, we need to ensure that the model outputs log-probabilities (using log_softmax in the final layer) and that the targets are formatted as probability distributions (using softmax). Using KLDivLoss with ReLU in the final layer will not work as expected, because ReLU does not produce a normalized probability distribution.\n",
    "\n",
    "With ReLU Activation: If using ReLU, the model cannot produce normalized probabilities, so KLDivLoss becomes meaningless because it compares distributions.\n",
    "\n",
    "With LogSoftmax Activation: Using log_softmax produces log-probabilities, which allows KLDivLoss to calculate divergence between the predicted and target distributions effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. CosineEmbeddingLoss (`torch.nn.CosineEmbeddingLoss`)\n",
    "- **Description:** Measures the cosine similarity between two input tensors, `x1` and `x2`, and computes the loss based on a label `y` that indicates whether the tensors should be similar (`y = 1`) or dissimilar (`y = -1`). Cosine similarity focuses on the angle between vectors, disregarding their magnitude.\n",
    "\n",
    "- **Mathematical Function:** \n",
    "\\begin{equation}\n",
    "  \\text{CosineEmbeddingLoss}(x1, x2, y) = \n",
    "  \\begin{cases} \n",
    "  1 - \\cos(x_1, x_2), & \\text{if } y = 1 \\\\\n",
    "  \\max(0, \\cos(x_1, x_2) - \\text{margin}), & \\text{if } y = -1\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "  where $ \\cos(x_1, x_2) $ is the cosine similarity between the two vectors, and `margin` is a threshold that determines how dissimilar the vectors should be.\n",
    "\n",
    "- **Use Case:** Commonly used in tasks like face verification, image similarity, and other scenarios where the relative orientation of vectors (angle) is more important than their length, such as in embeddings and metric learning.\n",
    "\n",
    "- **Background:** Cosine similarity compares the directional alignment of vectors, making it ideal for high-dimensional data where the magnitude may not be as informative. This loss is particularly useful when training models to learn meaningful embeddings that capture semantic similarity.\n",
    "\n",
    "You'll become more fimiliar with this loss function in future.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization in Machine Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Regularization is a fundamental technique in machine learning that helps prevent overfitting by adding a penalty to the loss function. This penalty discourages the model from becoming too complex, ensuring better generalization to unseen data. In this notebook, you will explore the concepts of regularization, understand different types of regularization techniques, and apply them using Python's popular libraries.\n",
    "\n",
    "## What is Regularization?\n",
    "\n",
    "Regularization involves adding a regularization term to the loss function used to train machine learning models. This term imposes a constraint on the model's coefficients, effectively reducing their magnitude. By doing so, regularization helps in:\n",
    "\n",
    "- **Preventing Overfitting:** Ensures the model does not become too tailored to the training data.\n",
    "- **Improving Generalization:** Enhances the model's performance on new, unseen data.\n",
    "- **Feature Selection:** Especially in L1 regularization, it can drive some coefficients to zero, effectively selecting important features.\n",
    "\n",
    "## Types of Regularization\n",
    "\n",
    "There are several types of regularization techniques, each imposing different constraints on the model's parameters:\n",
    "\n",
    "### 1. L1 Regularization (Lasso)\n",
    "\n",
    "L1 regularization adds the absolute value of the magnitude of coefficients as a penalty term to the loss function. It can lead to sparse models where some feature coefficients are exactly zero.\n",
    "\n",
    "### 2. L2 Regularization (Ridge)\n",
    "\n",
    "L2 regularization adds the squared magnitude of coefficients as a penalty term to the loss function. It tends to shrink the coefficients evenly but does not set them to zero.\n",
    "\n",
    "### 3. Elastic Net\n",
    "\n",
    "Elastic Net combines both L1 and L2 regularization penalties. It balances the benefits of both Lasso and Ridge methods, allowing for feature selection and coefficient shrinkage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Time!\n",
    "Import Iris dataset from sklearn.datasets and apply ridge regression with different alpha values. Then, create a gif that shows the changes of the classification boundary with respect to alpha values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libs that you need and start coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import imageio\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Iris dataset and select Setosa and Versicolor classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (70, 2)\n",
      "Testing set shape: (30, 2)\n",
      "Sample of standardized training features:\n",
      " [[1.94114606 1.29689348]\n",
      " [1.62587872 1.22635537]\n",
      " [2.2564134  1.43796969]\n",
      " [1.94114606 1.08527916]\n",
      " [2.09877973 1.36743159]]\n",
      "Sample of training labels:\n",
      " [1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# 1. Load and Prepare the Iris Dataset\n",
    "\n",
    "# Select only two classes for binary classification (Setosa and Versicolor)\n",
    "\n",
    "# Select two features for 2D visualization (Sepal Length and Petal Length)\n",
    "\n",
    "# Split into training and testing sets\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "selected_classes = (y == 0) | (y == 1)\n",
    "X = X[selected_classes]\n",
    "y = y[selected_classes]\n",
    "\n",
    "X = X[:, [0, 2]]  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n",
    "print(\"Sample of standardized training features:\\n\", X_train[:5])\n",
    "print(\"Sample of training labels:\\n\", y_train[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Function to Plot Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, alpha):\n",
    "    # Define the grid over the feature space\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "\n",
    "    # Predict over the grid\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = model.predict(grid)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Create a figure\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, levels=[-0.1, 0.1, 1.1], colors=['blue', 'red'])\n",
    "\n",
    "    # Scatter plot of the training data\n",
    "    scatter = ax.scatter(\n",
    "        X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolor='k', s=50\n",
    "    )\n",
    "\n",
    "    # Title and labels\n",
    "    ax.set_title(f'MLP Decision Boundary (alpha={alpha})')\n",
    "    ax.set_xlabel('Sepal Length (standardized)')\n",
    "    ax.set_ylabel('Petal Length (standardized)')\n",
    "\n",
    "    # Remove axes for clarity\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Tight layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot to a BytesIO object\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close(fig)\n",
    "    buf.seek(0)\n",
    "    return Image.open(buf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train MLP with Varying Alpha Values and Collect Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decision_boundary_gif(alpha_values, X_train, y_train, n_neurons):\n",
    "    # List to store images\n",
    "    images = []\n",
    "\n",
    "    for idx, alpha in enumerate(alpha_values):\n",
    "        print(f\"Processing alpha={alpha:.4f} ({idx + 1}/{len(alpha_values)})\")\n",
    "\n",
    "        # Create and train the MLP\n",
    "        mlp = MLPClassifier(hidden_layer_sizes=(n_neurons,), alpha=alpha, max_iter=1000, random_state=42)\n",
    "        mlp.fit(X_train, y_train)\n",
    "\n",
    "        # Plot decision boundary and get the image\n",
    "        img = plot_decision_boundary(mlp, X_train, y_train, alpha)\n",
    "        images.append(img)\n",
    "\n",
    "    # Save the images as a GIF\n",
    "    gif_filename = 'mlp_classification_boundaries_Rosa.gif'\n",
    "    images[0].save(\n",
    "        gif_filename,\n",
    "        save_all=True,\n",
    "        append_images=images[1:],\n",
    "        duration=500,\n",
    "        loop=0\n",
    "    )\n",
    "\n",
    "    print(f\"GIF saved as '{gif_filename}'\")\n",
    "\n",
    "    return gif_filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing alpha=0.0100 (1/30)\n",
      "Processing alpha=0.0144 (2/30)\n",
      "Processing alpha=0.0208 (3/30)\n",
      "Processing alpha=0.0299 (4/30)\n",
      "Processing alpha=0.0431 (5/30)\n",
      "Processing alpha=0.0622 (6/30)\n",
      "Processing alpha=0.0896 (7/30)\n",
      "Processing alpha=0.1291 (8/30)\n",
      "Processing alpha=0.1860 (9/30)\n",
      "Processing alpha=0.2681 (10/30)\n",
      "Processing alpha=0.3863 (11/30)\n",
      "Processing alpha=0.5567 (12/30)\n",
      "Processing alpha=0.8022 (13/30)\n",
      "Processing alpha=1.1561 (14/30)\n",
      "Processing alpha=1.6660 (15/30)\n",
      "Processing alpha=2.4009 (16/30)\n",
      "Processing alpha=3.4599 (17/30)\n",
      "Processing alpha=4.9861 (18/30)\n",
      "Processing alpha=7.1854 (19/30)\n",
      "Processing alpha=10.3548 (20/30)\n",
      "Processing alpha=14.9223 (21/30)\n",
      "Processing alpha=21.5043 (22/30)\n",
      "Processing alpha=30.9897 (23/30)\n",
      "Processing alpha=44.6591 (24/30)\n",
      "Processing alpha=64.3578 (25/30)\n",
      "Processing alpha=92.7456 (26/30)\n",
      "Processing alpha=133.6549 (27/30)\n",
      "Processing alpha=192.6090 (28/30)\n",
      "Processing alpha=277.5673 (29/30)\n",
      "Processing alpha=400.0000 (30/30)\n",
      "GIF saved as 'mlp_classification_boundaries_Rosa.gif'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use np.logspace to generate alpha values from 0.01 to 400\n",
    "alpha_values = np.logspace(np.log10(0.01), np.log10(400), num=30)\n",
    "# Define the number of neurons in the hidden layer\n",
    "n_neurons = 10  # Example number of neurons, adjust as desired\n",
    "\n",
    "# Create the decision boundary GIF\n",
    "gif_filename = create_decision_boundary_gif(alpha_values, X_train, y_train, n_neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your gif should look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "### **Multilayer Perceptron Classification Boundaries**\n",
    "\n",
    "![Classification Boundaries](mlp_classification_boundaries_example.gif)\n",
    "\n",
    "*Figure 1: Demonstration of classification boundaries created by a Multilayer Perceptron (MLP) model.*\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
